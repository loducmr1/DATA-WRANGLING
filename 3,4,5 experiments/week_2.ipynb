{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a204a91-f9b6-46ab-8c04-97d1046d4b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2b99c-03e3-457f-8a73-dbf2ec621ef7",
   "metadata": {},
   "source": [
    "Develop python script to parse a pdf file using pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f7eb69-9cd4-4a5e-a2e9-9a2c61d1c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "#Extract the text from pdffile\n",
    "text=extract_text(\"201000462.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e27cd58-327e-441c-8c0f-291b9966d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "t\n",
      "c\n",
      "O\n",
      "1\n",
      "\n",
      "]\n",
      "L\n",
      "M\n",
      "\n",
      ".\n",
      "t\n",
      "a\n",
      "t\n",
      "s\n",
      "[\n",
      "\n",
      "1\n",
      "v\n",
      "2\n",
      "6\n",
      "4\n",
      "0\n",
      "0\n",
      ".\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "A SURVEY ON NATURAL LANGUAGE PROCESSING\n",
      "(NLP) & APPLICATIONS IN INSURANCE\n",
      "\n",
      "PREPRINT\n",
      "\n",
      "Antoine Ly\n",
      "Data Analytics Solutions\n",
      "SCOR\n",
      "Paris, France\n",
      "aly@scor.com\n",
      "\n",
      "Benno Uthayasooriyar\n",
      "Université de Bretagne Occidentale\n",
      "EURIA\n",
      "Brest, France\n",
      "uth.benno@gmail.com\n",
      "\n",
      "Tingting Wang\n",
      "Data Analytics Solutions\n",
      "SCOR\n",
      "Beijing, China\n",
      "twang@scor.com\n",
      "\n",
      "October 2, 2020\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Text is the most widely used means of communication today. This data is abundant but nevertheless\n",
      "complex to exploit within algorithms. For years, scientists have been trying to implement different\n",
      "techniques that enable computers to replicate some mechanisms of human reading. During the past\n",
      "ﬁve years, research disrupted the capacity of the algorithms to unleash the value of text data. It brings\n",
      "today, many opportunities for the insurance industry.\n",
      "Understanding those methods and, above all, knowing how to apply them is a major challenge and\n",
      "key to unleash the value of text data that have been stored for many years. Processing language with\n",
      "computer brings many new opportunities especially in the insurance sector where reports are central\n",
      "in the information used by insurers.\n",
      "SCOR’s Data Analytics team has been working on the implementation of innovative tools or products\n",
      "that enable the use of the latest research on text analysis. Understanding text mining techniques in\n",
      "insurance enhances the monitoring of the underwritten risks and many processes that ﬁnally beneﬁt\n",
      "policyholders.\n",
      "This article proposes to explain opportunities that Natural Language Processing (NLP) are providing\n",
      "to insurance. It details different methods used today in practice1 traces back the story of them. We\n",
      "also illustrate the implementation of certain methods using open source libraries and python codes\n",
      "that we have developed to facilitate the use of these techniques.\n",
      "After giving a general overview on the evolution of text mining during the past few years, we share\n",
      "about how to conduct a full study with text mining and share some examples to serve those models\n",
      "into insurance products or services. Finally, we explained in more details every step that composes\n",
      "a Natural Language Processing study to ensure the reader can have a deep understanding on the\n",
      "implementation.\n",
      "\n",
      "Keywords NLP · Machine Learning · Insurance · Python · BERT\n",
      "\n",
      "Introduction\n",
      "\n",
      "1 What is it about?\n",
      "\n",
      "Text mining is a ﬁeld related to data analytic consisting in the analysis of textual data. A textual data point can be a\n",
      "character, a word, a sentence, paragraph or a full document. As text data is unstructured (opposed to tabular data), it\n",
      "requires speciﬁc approaches and models to be able to use it.\n",
      "\n",
      "1And especially those implemented in the different tools we propose to our clients\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Most of the time, text is analysed by a human who can read the text and transform it into structured information. It can\n",
      "take a lot of time when it comes to analyzing thousands of sentences. Natural Language Processing (NLP) refers to\n",
      "the capacity of copying the human reading process. Both terms \"Text mining\" or \"Natural Language Processing\" are\n",
      "used in the industry to refer to text manipulation with algorithms (despite some differences between both concepts).\n",
      "More than just mining text, NLP also tries to capture the complexity of the language and translates it into summarized\n",
      "information that a computer could use.\n",
      "\n",
      "Nowadays, text is everywhere. It is the main information we all use. Text is present in every website, report, or any other\n",
      "digital mean of communication. Thus, Natural Language Processing paves the way towards numerous opportunities\n",
      "starting to be heavily used by the insurance industry as illustrated on Figure 1.\n",
      "\n",
      "Figure 1: Usage example of Natural Language Processing based on SCOR experience.\n",
      "\n",
      "2 Opportunities for the insurance industry.\n",
      "\n",
      "In insurance, NLP can be leveraged at many steps of a policy life cycle. We present below some examples. Details on\n",
      "some use cases are also presented later in this article.\n",
      "\n",
      "• Marketing: NLP has been used to enhance marketing actions for a couple of years by different insurance\n",
      "companies. At SCOR, we used it to monitor the sentiment analysis of comments to better consider insured\n",
      "needs or monitor the perception on a speciﬁc risk (e.g. what people are thinking about a particular product). It\n",
      "is common also to process feedback that people publish on different social networks such as Twitter, Reddit\n",
      "or press articles to extract some trends in expectations and then better pilot the strategy of the company.\n",
      "Automating comments analysis enable a live monitoring of what is published on the internet. NLP model can\n",
      "indeed convert text into a general sentiment score or extract the topic they are referring to. If the underlying\n",
      "NLP technique can be sophisticated and have been evolving, it performs pretty well and the challenge is more\n",
      "about integrating such models into operational daily business. We illustrate this challenge and some solutions\n",
      "in paragraphs below.\n",
      "\n",
      "• Underwriting: At the re-insurance level, underwriters need to analyse a large number of policies at each\n",
      "renewal period. After digitalization of the contract, many text mining applications are possible on the wordings\n",
      "in order to track changes and check compliance. It makes the underwriting process friction-less and enable a\n",
      "better monitoring and change tracking in clauses. Those automatic checks might also be useful for the insured\n",
      "to provide more transparency on the coverage of the policies from one period to another. In life insurance,\n",
      "many opportunities are also opened such as enhancing medical report analysis. By coupling Optical Character\n",
      "Recognition (OCR)2 and NLP we can indeed extract information from Medical reports and help underwriters\n",
      "to focus on most difﬁcult cases while others can take less time to process. In another coming paper, we will\n",
      "detail how for instance NLP can be used to automatically anonymize documents with Personal Information\n",
      "\n",
      "2A paper focusing of this topic will be released soon\n",
      "\n",
      "2\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "and then process data compliant with GDPR. We explain in this article one of the key NLP method that can be\n",
      "used for anonymization which is called Name Entity Recognition (NER).\n",
      "\n",
      "• Claims: Textual analysis of claims and classiﬁcation are two topics where SCOR has been using NLP. It can\n",
      "simplify acceptance of claims in the process to reduce time treatment, operational errors or even help in fraud\n",
      "detection. We detail below how text mining can be used for claims adjudication or to optimize human reading\n",
      "to classify claims and route them to the appropriate department.\n",
      "\n",
      "• Reserving: Analyzing comments of claim reports during the First Notiﬁcation of Loss or expert claims\n",
      "assessment is more and more developed in insurance company to improve reserving for severe claims.\n",
      "Descriptions can provide valuable information to anticipate the development of claim and better estimate the\n",
      "expected cost. In life insurance, the assessment of expert reports could help in reserve projection. It could for\n",
      "instance help in Long Term Care or speciﬁc Critical Illness products where risk factors can be cross correlated\n",
      "to different dependent diseases. Despite the low annual frequency of such reports, the amount of document\n",
      "can become signiﬁcant on a long time study period for which a systematic approach is useful.\n",
      "\n",
      "• Prevention: Enhancing Medical Diagnostic is another opportunity for NLP. It could help doctors to ensure\n",
      "they cover all possible diseases matching with symptoms. In this particular ﬁeld, the challenge is to train\n",
      "NLP models on medical data that have a speciﬁc vocabulary. Early diagnostic is really key to ensure good\n",
      "health of people and avoid complications. Some systems already exists to capture different ﬁelds but they are\n",
      "quite time consuming. Doctors have to ﬁll many ﬁelds that are structured in templates and most of the time\n",
      "free comments are not allowed (while they are the most easy to use). In such application, NLP models must\n",
      "be applied in compliance with highly secured systems (anonymization of data, local computation, etc.). We\n",
      "detail further in this article some technologies that can be used to embed NLP model into solutions and help in\n",
      "respecting such constraints.\n",
      "\n",
      "Manipulating text, provides many new opportunities in the insurance industry. We explain further in this paper how\n",
      "to apply the different techniques of Natural Language Processing in order to deliver business use cases. The article\n",
      "gradually explains the different steps that are commonly used in text manipulation and illustrates how new state-of-the\n",
      "art NLP models can help insurance industry to process for instance claims description, or to adjudicate claims based on\n",
      "report and then optimize payment process (by focusing investigation on most relevant cases). However, if research has\n",
      "been doing tremendous progress in text manipulation, it is not without challenges to apply the different techniques to\n",
      "the industry and provide NLP based product.\n",
      "\n",
      "3 Main challenges to embed NLP models into solutions.\n",
      "\n",
      "3.1 How to access or collect useful text data?\n",
      "\n",
      "Text data are almost everywhere. We use them to communicate and send information that are stored into different layers:\n",
      "emails, reports, websites, information systems, logs, etc. However, they are also one the less exploited information\n",
      "by industries. If data are stored everywhere, gathering them (i.e. relevant text information) can be a real challenge.\n",
      "In legacy industries like insurance, text data are stored within paper documents that most of the time are scanned.\n",
      "Capturing and extracting those information is the ﬁrst challenge to be able then to use text into NLP models. Some\n",
      "methods based on Optical Character Recognition (OCR) that SCOR implemented to overcome that ﬁrst extraction step\n",
      "will be detailed in another paper coming in Q4 2020.\n",
      "\n",
      "For most recent digital systems, text is easier to access and can be either collected directly from databases lying\n",
      "behind applications, websites, social media or by simply converting documents (like pdf generated by an information\n",
      "system). When it comes to collecting data from social media, like Twitter, Reddit, etc. those are usually providing some\n",
      "connectors (APIs) that allow to collect information in a more structured format. In any cases, as for regular tabular data,\n",
      "the collect of text must be driven by an initial business issue. Collecting text information without restriction can indeed\n",
      "lead to bias and noise that algorithms might struggle to deal with since relations between text and business issue will be\n",
      "harder to detect.\n",
      "\n",
      "3.2 Enrich traditional information to enhance services.\n",
      "\n",
      "In practice, text data are mainly used in addition to more traditional tabular data. For example, to speed up payment\n",
      "process, free text that describes the claim can be added to tabular data that contain more traditional information (policy,\n",
      "claim date, type, etc.). Text is usually really rich information that humans process to take decision or to explain context.\n",
      "This speciﬁc context is most of the time a piece of information that is difﬁcult to capture within structured data.\n",
      "\n",
      "3\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Since text is unstructured, NLP helps in creating numerical representation of language so it can be merged to standard\n",
      "tabular information. The demand in the insurance market in the usage of text is high. As one use case, we present in the\n",
      "following an application which was developed to showcase the added-value of NLP (deep learning) algorithms in\n",
      "the classiﬁcation of claims and SCOR capacity to interface such models with insurance practitioner. The following\n",
      "example shares how to automate Critical Illness Claims classiﬁcation and achieve performance equivalent (if not\n",
      "even better) to human practice. The usage of NLP models increases the efﬁciency of claim analysis and accelerates\n",
      "the claims classiﬁcation process. Thus with 20 000 reports, our algorithm classiﬁed accurately 90% of them (cf Figure 2).\n",
      "\n",
      "The ﬁtted algorithm can then be used to process the full history of claims and to monitor underwriting network\n",
      "distribution and the evolution of claims by location.\n",
      "\n",
      "Figure 2: Critical illness classiﬁcation use case\n",
      "\n",
      "NLP models integration helps not only the insurance company to get a better insights on their claims but also allows to\n",
      "remove friction in the claim payment process for regular claims. With the apparition of more and more digital insurance\n",
      "during the past few years, increasing every step of an insurance product life cycle became necessary. NLP is deﬁnitely a\n",
      "key asset here to better use free text and enhance the reactivity of insurers.\n",
      "\n",
      "3.3\n",
      "\n",
      "Interfacing NLP models.\n",
      "\n",
      "When it comes to use machine learning models, integrating them into real products can be challenging. To overcome the\n",
      "proof of concept, models need to be integrated in product pipelines and be monitored properly. For end users, models\n",
      "need to be used through simple interface that can make decisions easier but also transparent enough to avoid any black\n",
      "box effect.\n",
      "\n",
      "Figure 3 illustrates the web interface that has been developed to classify claims. More speciﬁcally, the model was\n",
      "integrated in one back-end application and we then developed a tailored full-stack, secure and cloud based solutions for\n",
      "our customer (following state-of-the art practices detailed below).\n",
      "\n",
      "It ensures end users to control the classiﬁcation and to use the model to label historical data. Such an interface\n",
      "is particularly useful for instance if we want to visualize and monitor speciﬁc distribution networks and study if\n",
      "underwriting can be improve in speciﬁc locations.\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Figure 3: Web application for critical illness classiﬁcation\n",
      "\n",
      "In practice, in order to setup the environment, SCOR Analytics team is using Docker. It makes it easier to create, deploy,\n",
      "and run applications by using containers. Docker is an open source technology that can be seen as a lightweight virtual\n",
      "machine. One Docker container can be used for the machine learning algorithm development: data preprocessing,\n",
      "embedding and modeling, ML pipelines, results analysis. It helps in tracking the environment and reviewing. Once the\n",
      "complete pipeline is in place, the trained model is saved as well as its environment. Thus, for the claim classiﬁcation\n",
      "use case, we embedded a machine learning model into another container, using the Python library Flask3 to build a Web\n",
      "API (Application Programming Interface). It allows the user to access the pipeline with their own data by making an\n",
      "HTTP request. It’s also possible to develop a user-friendly graphical Web application using JavaScript frameworks like\n",
      "Angular4 or React5.\n",
      "\n",
      "Figure 4: User ﬂow diagram\n",
      "\n",
      "The Figures 4 and 5 illustrates the logical view of the process and architecture used to ensure modular implementation\n",
      "of the model. This architecture is not speciﬁc to text mining models but illustrate more broadly what technologies can\n",
      "be used to deploy smoothly machine learning models into an existing process.\n",
      "\n",
      "3https://flask.palletsprojects.com/en/1.1.x/\n",
      "4https://angular.io/\n",
      "5https://reactjs.org/\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Figure 5: Deployment architecture diagram\n",
      "\n",
      "3.4 Some new opportunities with latest releases in NLP?\n",
      "\n",
      "Along the previous paragraphs we mentioned sentiment analysis and claims classiﬁcation as some examples. SCOR\n",
      "has been using text mining on multiple lines of business for couple of years now. It brings value not only for internal\n",
      "process but also to insured since it speed-up processes.\n",
      "\n",
      "Another example is the claims adjudication. In this use case, text mining is used to strengthen the traditional tabular\n",
      "data. By coupling text embedding to claims proﬁle, we designed an algorithm able to speed-up payments. A score is\n",
      "used to design 3 priority categories (High, Medium, and Low) and help claim managers to prioritize claims investigation\n",
      "on complex cases and automatize payment for regular claims.\n",
      "\n",
      "Since text is available almost everywhere, the opportunities that bring NLP are really massive. The release in 2019 of\n",
      "latest text mining models brings new opportunities of services. Among those models, BERT (see section 36 page 27 for\n",
      "more details) models6 can for instance be used for direct information extraction through Question Answering (Q&A).\n",
      "In the original paper presenting BERT [Jacob et al. 2018], one of the task it was ﬁne-tuned for was Q&A, using the\n",
      "Stanford Question Answering Dataset (SQUAD) speciﬁcally developed for this task (see details about BERT section\n",
      "\n",
      "6Initial BERT and all its extension\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "12.5 page 26). The SQUAD consists of various English texts. With each document of the data set comes questions that\n",
      "are answered with an extract from the document.\n",
      "\n",
      "Figure 6:\n",
      "SQuAD-explorer/\n",
      "\n",
      "Extract of\n",
      "\n",
      "the SQUAD taken from the source website https://rajpurkar.github.io/\n",
      "\n",
      "One application of Question Answering for insurance companies is information retrieval from medical surveys. This\n",
      "example was developed in our library demonstrating Q&A on different types of texts. After the release of the ﬁrst\n",
      "BERT model, many engineers came up with their own versions of the model by adapting the original architecture. In\n",
      "this example we use a french version called CamemBERT. The version we use is one that was ﬁne-tuned by French\n",
      "company Illuin’s engineers on a dataset for Question Answering they built on their own called FQUAD (French Question\n",
      "Answering Dataset) [d’Hoffschmidt et al. 2020].\n",
      "\n",
      "Figure 7: Question Answering on a fake medical survey that can be used for automatic de-identiﬁcation. If the answer\n",
      "given by the model is not correct, we display the top k answers to examine the other candidate answers.\n",
      "\n",
      "The model proved to be efﬁcient on simple and explicit questions, however, as shown ﬁgure 7, when asked for the\n",
      "profession of the fake person, it gives out the wrong answer (médecin, which translates to doctor). This can be\n",
      "understood as the word médecin appears a lot throughout the survey. A solution to have better results and use the model\n",
      "for more complex medical and insurance related questions could be to ﬁne-tune the model one more time, on a dataset\n",
      "of medical surveys and insurance documents.\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "4 Some history to understand evolution of NLP and new opportunities\n",
      "\n",
      "Even if NLP is more and more popular nowadays, it is not a so young discipline. The ﬁrst automatic extraction of\n",
      "information from text dates back to the 1970s with the work of DeJong (1979). Most often referred to as Natural\n",
      "Language Processing (NLP), semantic analyzes were subsequently developed in the 1990s [Grishman and Sundheim\n",
      "1996] with the creation of the Message Understanding Conference (MUC). If research has intensiﬁed since then,\n",
      "the MUC initially proposes extracting linguistic information by using templates in which ﬁelds can be ﬁlled. Very\n",
      "useful to collect information in administration, once extracted, text can automatically ﬁll other templates according\n",
      "to the information extracted from the values indicated by the user (e.g. generation of contract, certiﬁcate, etc.). In\n",
      "this approach, the structure of the response is assumed to be known. So in some ﬁelds, a simple common name is\n",
      "expected while in others, a sentence or adjectives can be ﬁlled. The extraction of information from templates therefore\n",
      "presupposes that we know in advance the structure of the information that will have to be extracted (for example a\n",
      "postal code in an address, an entity name or even a relationship). This approach is thus very limited in many situations.\n",
      "Less restrictive alternatives have appeared in order to be able to extract information from free text. Particularly favored\n",
      "by the development of the internet and the growth in the number of websites full of textual information [Aggarwal and\n",
      "Zhai 2012b], the researchers are initially interested in using semi-structured text in HTML format [Banko et al. 2007].\n",
      "We therefore distinguish two major tasks which form the basis of the NLP [Jiang 2012]:\n",
      "\n",
      "• Recognition of the name of an entity designated most of the time by the acronym NER (Name Entity\n",
      "Recognition): these techniques are concerned with extracting, from a text, the value of a common or proper\n",
      "name but also extends to the recognition of metric values (date, price, etc.).\n",
      "\n",
      "• The extraction of relations, referring to techniques trying to determine the logical links between the different\n",
      "words of a text. From a raw or semi-structured text, the challenge is to ﬁnd the grammatical relationships\n",
      "between the terms.\n",
      "\n",
      "4.1 Name Entity Recognition\n",
      "\n",
      "The easiest way to implement name entity recognition is to use rules. When a pattern is detected, a rule is applied. The\n",
      "use of regular expressions [Califf and Mooney 1999] makes it possible to extract textual information which follows a\n",
      "particular format. For example, \"Mr /. (. *)\" allows you to look for all the texts starting with \"Mr\" and to be interested\n",
      "in the group which follows it which is supposed to represent the name of the person. More precise rules allow to specify\n",
      "the authorized character values with which the pattern is supposed to correspond. Rules by regular expressions are\n",
      "nowadays commonly used in industry and are integrated in many products.\n",
      "\n",
      "Other approaches like statistical learning approaches exist. In this context, the general idea is to be able to assign to any\n",
      "word (making up one document), one category indicating whether the word represents a particular entity to be extracted\n",
      "or not. In other words, statistical learning addresses the question of the recognition of entities through the form of a\n",
      "classiﬁcation problem. As indicated by Jiang (2012), in this situation, the labels associated with the different words\n",
      "generally follow a convention called BIO [Rau 1991].\n",
      "\n",
      "Figure 8: Illustration of categories following BIO convention\n",
      "\n",
      "Rabiner (1989) and Bikel et al. (1997) were pioneers in the study of NER algorithms by looking at the probability that\n",
      "one word is from a given category knowing the other words of the document. Bikel et al. (1997) suggests in particular a\n",
      "supervised learning approach in order to determine the role of each word in a document. This approach is very similar\n",
      "to many implementations of text mining models.\n",
      "\n",
      "4.2 The study of relationships\n",
      "\n",
      "Other methods consist in deﬁning relations between the extracted entities. For example, in the sentence \"SCOR Data\n",
      "Analytics team has been deploying services using text mining models\", extracting entities would lead to deﬁning \"SCOR\n",
      "Data Analytics\" as a proper noun and \"services\" as a direct object. However, the extraction of entity name does not\n",
      "make it possible to infer that \"SCOR Data Analytics team\" is the team which deploys the \"service\". This relationship\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "study task can be approached as a classiﬁcation problem between two co-occurring entities. Jiang and Zhai (2007)\n",
      "introduce a systematic approach by representing the relationship between the entities by a directed graph.\n",
      "\n",
      "Figure 9: Example of dependence tree introduced by Jiang and Zhai (2007)\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Following the idea of graphical representation of the connections between entities and of being able to extract\n",
      "subgroups of connected entities, kernel methods based on trees are proposed by Zelenko et al. (2003). These methods\n",
      "will be extended in particular with the application of convolutional trees [Zhang et al. 2006]. Convolutions allow a\n",
      "representation of a vector in a new space. In a convolutional tree, each dimension of the vector can then be associated\n",
      "with a subset of the tree and Jiang and Zhai (2007) then exposes examples of the use of this method.\n",
      "\n",
      "If the methods have evolved a lot since the early 2000s, the desire to establish relationships between words - and thus\n",
      "adopt a grammatical approach to semantic analysis - remains valid. More recently, Speer et al. (2017) offer a general\n",
      "representation of writing and in particular of language, also using graph theory. The result, called ConceptNet 7 is thus a\n",
      "logical graph which connects words or sentences. Its construction is based on data provided by experts and others from\n",
      "questionnaires. The motivation is thus to provide a new representation of the language and improve the understanding\n",
      "of the language [Speer et al. 2017]. The constitution of this graph has been greatly favored by the development of\n",
      "parallel computing techniques and the amount of data available. Thus the nodes (or the words) are put in relation by\n",
      "connections like: \"Is used for\", \"Is capable of \", \"Is a\". Figure 10 illustrates an extract.\n",
      "\n",
      "Figure 10: ConceptNet\n",
      "\n",
      "4.3 The statistical approach and the use of Machine Learning\n",
      "\n",
      "If the extraction of information from free text makes it possible to respond to the detection of entity names or the study\n",
      "of the relationships between terms, the use of text can also be done in order to solve regression problems or more\n",
      "\n",
      "7http://conceptnet.io/\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "standard classiﬁcation [Aggarwal and Zhai 2012a]. Rather than considering words as entities related by attributes,\n",
      "one possibility is to simply consider words as occurrences of a random variable. Similar to the method introduced in\n",
      "the statistical approach allowing the recognition of entities, the documents are then described by statistics [Nenkova\n",
      "and McKeown 2012] without however taking their order into account. The idea is to consider that words frequently\n",
      "appearing together in a document reveal information.\n",
      "\n",
      "Thus, a ﬁrst statistical representation of the text consists in representing each document by counting words occurrences\n",
      "according to a dictionary. The resulting counting vectors dimensions then represent a word from the dictionary (see\n",
      "section 10 page 19 for more illustrations). In this representation, called Bag of Words (BoW), the idea consists in\n",
      "simply summing the canonical vectors associated with the words making up the sentence. In practice, the dictionary is\n",
      "rarely deﬁned as the entirety of the words of a language. It is made up of the words observed in the corpus of documents\n",
      "used in the study.\n",
      "\n",
      "This representation in Bag of Words, though it is simple, has the disadvantage of not being sparse. Indeed, in a sentence,\n",
      "we generally use only few words among those existing in the dictionary. On the other hand, articles, linking words\n",
      "or common terms (called stopwords) are often over-represented by this method. To cope with this last drawback, a\n",
      "heuristic consists in ﬁxing a certain list of unwanted words and removing them from the dictionary.\n",
      "\n",
      "In a speciﬁc set of documents -called corpus, the frequency of certain words may also be higher than in others. Thus, an\n",
      "alternative to the representation in a bag of words is one which no longer simply counts the occurrence of a word in the\n",
      "document but also takes account of its appearances in the corpus.\n",
      "\n",
      "Text mining and NLP research over the past 10 years has attempted to overcome the various limitations of early\n",
      "historical methods. The past ﬁve years have disrupted the state-of-the art text mining models by using deep learning\n",
      "[Mikolov et al. 2013] [Vaswani et al. 2017] [Jacob et al. 2018]. The most recent models are able with high precision to\n",
      "capture not only words interaction but are also able to adapt the focus on speciﬁc words according to the context of\n",
      "the sentence (cf section 32). Those most recent NLP models provide new opportunities in client relationship but also\n",
      "improvement in document analysis.\n",
      "\n",
      "The following sections help in understanding how to use the different machine learning models with text data and apply\n",
      "them to industrial cases. We particularly focus on the latest NLP models but reader might refer to [Charpentier et al.\n",
      "2018] for a reminder on more traditional machine learning models mentioned in this article that can be combined with\n",
      "NLP models.\n",
      "\n",
      "NLP in practice: from data processing to production\n",
      "\n",
      "In practice, text manipulation follows standard pipelines in order to answer the initial business issue motivating the\n",
      "usage of NLP. In this section, we illustrate the different steps that are applied in practice when manipulating text data.\n",
      "Some illustrations are given in Python and based on the Twitter’s Tweet for Sentiment Extraction Dataset (cf Figure 11)\n",
      "or on other use cases to illustrate some speciﬁc steps. This section provides a high level overview of techniques and are\n",
      "addressed to non-expert readers. Deeper explanations for each step of the process are available in section 10 page 16.\n",
      "\n",
      "5 Text mining illustrated through use cases\n",
      "\n",
      "If every project has its own speciﬁcity (underwriting, claims analysis, fraud detection, etc.), there are some steps that\n",
      "are common when it comes to manipulating text. For this reason, a Python library has been developed by SCOR Data\n",
      "Analytics team to facilitate the use of text mining8. The developed library reﬂects each of standard step to process text.\n",
      "It embeds some open source libraries and adds some customized features that help dealing with different languages and\n",
      "make usage uniform.\n",
      "\n",
      "To decrease the learning curve of some functionalities, a few notebooks have been created, involving some preprocessing\n",
      "steps (implemented in the module ‘preprocessing‘ as illustrated in Figures below). For instance, the notebook entitled\n",
      "[TUTO]Data_analysis, presents and end-to-end study applied on a dataset of tweets along with a sentiment score\n",
      "(negative, neutral or positive), from the Tweet Sentiment Extraction9 Kaggle competition.\n",
      "\n",
      "8Please contact authors for more information\n",
      "9https://www.kaggle.com/c/tweet-sentiment-extraction/data?select=test.csv\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "Figure 11: Extract of Kaggle’s Tweet for Sentiment Extraction Dataset.\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "In the following paragraphs, this example is taken as an illustration for the different transformations that allow text\n",
      "manipulation by algorithms.\n",
      "\n",
      "6 Data is never clean: the preprocessing\n",
      "\n",
      "Some words are not useful in term of predictive power because they are too frequent, therefore not very discriminant.\n",
      "The goal of the preprocessing is to prepare text representation so it can feed a parametric models (e.g. linear regression)\n",
      "by cleaning the data. Indeed, all strings must have a similar form and unnecessary elements must be removed from our\n",
      "inputs since they can introduce noise. The most essential preprocessing steps are:\n",
      "\n",
      "• Loading the text: It is common, when dealing with speciﬁc characters like accents to face encoding issues.\n",
      "This part aims to solve text representation and encoding problems using the \"unicodedata\" library. Encoding\n",
      "conversion from Latin1 to UTF8 (e.g. when dealing with mix of French and English words) can be applied\n",
      "before processing the text.\n",
      "\n",
      "• Lowercasing: It is the operation of converting all letters to lowercase. This will get rid of any ambiguity\n",
      "between lower-case and upper-case letters. This step can easily be applied with built-in methods associated to\n",
      "string objects (\".lower()\" for instance in Python).\n",
      "\n",
      "• Stopwords removal: When communicating, humans must use words such as “the”, “and” or “a” for sentences\n",
      "to make sense, however these words do not carry any meaning and are not useful for data analysis. Getting\n",
      "rid of these words called stopwords is a preprocessing step that allows focusing on words that matter. Each\n",
      "language possesses its own list of stopwords that can be adjusted according to the problem. The ﬁrst idea\n",
      "of stopwords appeared in an article by Luhn (1960). Lists of predeﬁned stopwords for many languages are\n",
      "available in NLTK10.\n",
      "\n",
      "• Special characters processing: an attention is brought to special characters. Indeed, while some of them are not\n",
      "necessary, others are needed for sentiment analysis. For instance, an exclamation point in a tweet can express\n",
      "an emphasis on its sentiment. People tend to increase the number of exclamation points when dealing with\n",
      "strong sentiments. Therefore, punctuation characters can be kept or treated as stopwords. The \"re\" library is\n",
      "used to remove duplicated punctuation symbols as well as other superﬁcial pieces of text like URLs or Twitter\n",
      "nametags thanks to regular expressions (see the beginning of section 4.1 page 8 for more information about\n",
      "regular expressions).\n",
      "\n",
      "• Tokenization: In order to study a text, the data must be split into sentences and possibly into words. This\n",
      "operation of splitting a text into smaller parts is called tokenization. A lot of methods cover tokenization,\n",
      "from Stemming [M.F.Porter 1980] to WordPiece [Schuster and Nakajima 2012]. See section 11.1 page 16\n",
      "for further details. In practice, libraries differ according to language for stemming (e.g. Jieba for Chinese,\n",
      "Treetagger for French or NLTK for English). Recently, models like BERT detailed in section 10 page 27 can\n",
      "deal with multi-language data with a unique model.\n",
      "\n",
      "10https://www.nltk.org/\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "As preprocessing is utterly basic, it is only natural to standardize it using the previously cited libraries within simple\n",
      "functions, which is what we have done to simplify preprocessing in text mining projects and really focus on the business\n",
      "issue. Thus, the preprocessing steps are adapted to any language using latin characters (French, English, German, etc.)\n",
      "as well as Chinese, and can be conveniently used by calling functions from our own library. We invite readers to contact\n",
      "authors for any sharing.\n",
      "\n",
      "Figure 12 and 13 illustrate some results of preprocessing raw tweets from the Tweet for Sentiment Extraction Dataset\n",
      "using SCOR library:\n",
      "\n",
      "Figure 12: Word level preprocessing.\n",
      "\n",
      "Textual data may have spelling or grammar mistakes. In this situation, Character Level preprocessing can be useful. It\n",
      "is particularly necessary when working on tweets.\n",
      "\n",
      "Figure 13: Character level preprocessing.\n",
      "\n",
      "In practice, the preprocessing is usually adapted to the considered problem but also to the language. For instance,\n",
      "Chinese characters need to be split in a speciﬁc manner as shown below, because the fact that words are not separable\n",
      "by single character prevents from using the same preprocessing method as in latin language.\n",
      "\n",
      "Figure 14: Chinese data preprocessing.\n",
      "\n",
      "7 Converting text to numerical vectors: Text Embedding\n",
      "\n",
      "Once the data is clean, the next step is transforming textual data into numerical vectors, through an operation that is\n",
      "called text embedding. Indeed, a computer cannot understand raw words and learn from them. It can only work with\n",
      "numbers. Therefore, an appropriate representation with numbers must be adopted.\n",
      "\n",
      "First a dictionary is built with the available vocabulary. Then, given a sentence from the dataset, each word/character is\n",
      "referenced to as a token, according to its rank in the dictionary. This process is also called tokenization, however it is\n",
      "not the same process as deﬁned in section 6 page 11. The two deﬁnitions are sometimes wrongly mixed-up. In order to\n",
      "avoid any confusion, we will talk about vocabulary tokenization when mentioning lexical indexation.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Figure 15: Dictionary used for vocabulary tokenization.\n",
      "\n",
      "As shown below, this method transforms textual data into a numerical vector.\n",
      "\n",
      "Figure 16: Vocabulary tokenization of a Chinese sentence.\n",
      "\n",
      "In the example shown ﬁgure 16, the tokenized vector is completed with \"0\". This is done because every tokenized\n",
      "vectors are required to have the same size. Usually, it is set to the size of the longest availabe tokenized sentence.\n",
      "\n",
      "After tokenizing words into indexes of a dictionary, other transformations are applied, taking factors like context\n",
      "or semantics into consideration. Several methods cover these transformation processes. One of the most famous\n",
      "methodologies is \"Word2Vec\" [Mikolov et al. 2013] which is detailed in section 12 page 19 with other alternative\n",
      "models. Text embedding is the combination of vocabulary tokenization and other transformations. It is common in the\n",
      "industry to leverage on pre-trained models to face lack of data in training set or to reduce computation times. The most\n",
      "popular pre-trained NLP embedding models are details in section 12 as well.\n",
      "\n",
      "8 Feature engineering.\n",
      "\n",
      "To come back on the previous example on Sentiment Analysis, it is also common and useful to create new features that\n",
      "could bring information. For instance, length of one’s comment, the count of some speciﬁc punctuation, etc. It is quite\n",
      "difﬁcult to provide an exhaustive list of feature engineering that can be applied to text since it is usually adapted to a\n",
      "speciﬁc issue.\n",
      "\n",
      "Some tricks can however be useful in the learning phase of an NLP model. Among them is the data augmentation that\n",
      "can be done thanks to translators. Coupling translation algorithms can help in enriching your data set and keep the mean-\n",
      "ing of your text. For instance, translating from English to French then French to English through another translator will\n",
      "return a similar sentence but not exactly the same. The longer are the paragraph you translate, the more useful is that trick.\n",
      "\n",
      "Other feature engineering methods like special character interpretation can help in pre-processing your text. When you\n",
      "deal with tweeter for instance or conversational text, interpreting emoji might provide relevant information. Converting\n",
      "them into words before manipulating the full text can sometimes enhance the information quality.\n",
      "\n",
      "13\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "For more speciﬁc use cases, like in life insurance, injecting some knowledge around medical vocabularies can also help\n",
      "in gathering similar words (e.g. diseases).\n",
      "\n",
      "9 Modelling with NLP.\n",
      "\n",
      "Once an input is embedded in the form of a numerical vector, a model has to be trained to deal with the main task.\n",
      "\n",
      "Figure 17: Usual strategy when dealing with common datasets where NLP is not the main task, but still required for\n",
      "efﬁcient text embedding.\n",
      "\n",
      "In the case of tweet sentiment extraction, after applying an efﬁcient text embedding, the modeling part would address\n",
      "the classiﬁcation problem. To each tweet, a sentiment has to be correctly attributed. Normally, the data preparation and\n",
      "modeling part are done separately. However, when the data is only textual, BERT can be used for both (see section 12.5\n",
      "page 26 for more details).\n",
      "\n",
      "In practice it is also common to merge numerical embedding vectors with regular tabular data in order to enrich\n",
      "the available features. For instance, in traditional insurance data, for each policy, the description could be enriched\n",
      "with columns that contain text information. Thus numerical added features can then feed the training of a speciﬁc model.\n",
      "\n",
      "After training, the model requires performance evaluation, which can be done through the choice of suitable metrics\n",
      "(see section 13) and rigorous study of interpretability.\n",
      "\n",
      "10 How to test and analyze a text mining model?\n",
      "\n",
      "NLP models are tools to enhance business products and help in solving issues that tabular data alone cannot solve. In\n",
      "more traditional predictive analytics only based on tabular data and statistical models, the interpretation of the model\n",
      "usually takes large part of the model analysis. Since models are used to better understand phenomenons (and then better\n",
      "anticipate them), matching the expert judgements with the model outcome is key to ensure relevance of the predictive\n",
      "model. It ensures the control but also the interpretation.\n",
      "\n",
      "In machine learning, there is a trade-off between model complexity and model interpretability. Complex machine\n",
      "learning models e.g. deep learning perform better than interpretable models. However, complexity can sometimes lead\n",
      "to black box effects. Research has been active for the past few years in providing additional instruments to open those\n",
      "black box models [Delcaillau et al. 2020].\n",
      "\n",
      "14\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "For simple vector embeddings like BoW, Tf-Idf (cf section 12 page 19 for more details), it is easy to use LIME or\n",
      "SHAP on top of the machine learning models to get some insights of the results. LIME stands for Local interpretable\n",
      "model-agnostic explanations. The idea behind this is to have a local linear approximation of the complex model.\n",
      "Alternatively, SHAP (SHapley Additive exPlanations) uniﬁes all available frameworks for interpreting predictions and\n",
      "it is based on the game theory concept. LIME and SHAP models are surrogate models that model the changes in the\n",
      "prediction. However it is important to note that these two methods are univariate and do not take into account the\n",
      "correlations between variables. Many other interpretation tools can then be added in complement to the embedding\n",
      "methodology and the model used to achieve a speciﬁc issue. The following ﬁgures illustrate SHAP and LIME’s\n",
      "explanation for a positive tweet11.\n",
      "\n",
      "Figure 18: Results analysis with LIME explainer in Tweet Sentiments Predictions\n",
      "\n",
      "Figure 19: Results analysis with SHAP explainer in Tweet Sentiments Predictions\n",
      "\n",
      "The above explanation shows features each contributing to push the model output from the base value (the average model output\n",
      "over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the\n",
      "prediction lower are in blue (these force plots are introduced in the Nature BME paper [Lundberg et al. 2018])\n",
      "\n",
      "With more complex word representations such as BERT, algorithm will have a better accuracy, but there is less possibility\n",
      "to explain each output individually. However, it is still possible to directly analyze the prediction and explain it from\n",
      "\n",
      "11Two tutorial notebooks are available in our library, [VisualizationNotebook] Explain model outputs LIME.ipynb and [Visualiza-\n",
      "\n",
      "tionNotebook] Explain model outputs SHAP.ipynb.\n",
      "\n",
      "15\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "text. For instance, one can do a word frequency analysis and get a quick look at a word cloud per classes. Whatever the\n",
      "model, we can always use common sense.\n",
      "\n",
      "Figure 20: Word cloud of the class rejection\n",
      "\n",
      "Further description of NLP\n",
      "\n",
      "11 Deeper understanding of text preprocessing\n",
      "\n",
      "11.1 From lowercasing to tokenization\n",
      "\n",
      "As explained in the previous chapter, words need to be treated before being converted to numbers and processed by a\n",
      "model. In order to do this, after a simple lower casing, a sentence is tokenized into smaller parts (often into words using\n",
      "a well thought regular expression), and the words are regrouped in a list. In the example of sentiment analysis, some\n",
      "punctuation characters like \"!\" are useful, as they can express a sentimental emphasis, therefore they are kept during\n",
      "this step. However, URLs are removed during this phase, as they provide no useful data for this speciﬁc analysis.\n",
      "\n",
      "Figure 21: Simple tokenization of a tweet.\n",
      "\n",
      "There are different methods of tokenization. The one illustrated ﬁgure 21 is tokenization of a sentence into words.\n",
      "However, words can also be divided into smaller parts. The point of tokenizing words is to ﬁnd similarity within words\n",
      "with the same roots. In order to understand this, two popular methods called Stemming and Wordpiece tokenization can\n",
      "be studied.\n",
      "\n",
      "11.1.1 Stemming\n",
      "\n",
      "Grammar rules often demand some sufﬁxes to be added to words for a sentence to make sense, however those sufﬁxes\n",
      "are not imperatives when it comes to understanding the meaning of a word. Stemming is a preprocessing step that gets\n",
      "rid of the unneeded parts of a word while keeping its root, also called the \"stem\".\n",
      "\n",
      "16\n",
      "\n",
      "\f",
      "Figure 22: An example of stemming. Illustration by Ganesan (2019).\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "In order to achieve such a result, two algorithms are mainly used: the Porter Stemming Algorithm, introduced by\n",
      "M.F.Porter (1980) and the Lancaster Stemming Algorithm (also known as Paice/Husk Stemming Algorithm) developed\n",
      "at Lancaster University by Hooper and Paice (2005). The Porter Stemming Algorithm is made of 5 steps or set of rules\n",
      "based on the structure of words that are applied in a speciﬁc order. These rules mainly focus on the combination of\n",
      "speciﬁc vowels and consonants. The Lancaster Stemming Algorithm is also based on a set of rules deﬁned to remove\n",
      "sufﬁxes, but though it is less complex and faster, this algorithm is iterative and can cause over-stemming, returning\n",
      "stems that are difﬁcult to understand.\n",
      "\n",
      "11.1.2 Lemmatization\n",
      "\n",
      "While stemming rids a word of sufﬁxes and preﬁxes and only keeps its root, lemmatization is an algorithm that replaces\n",
      "a word by its most basic form, also called \"lemma\". A lemma can be an inﬁnitive form, a noun, an adjective, etc, while\n",
      "a stem often means nothing. This is useful because in some languages, words with different meanings can have the\n",
      "same stem. Therefore, lemmas aim to convey an idea with an actual word instead of a basic stem with no deﬁnite\n",
      "meaning, correcting the problem of having two different concepts with a same root. Lemmatization thus requires a\n",
      "morphological analysis of the word and the existence of a detailed dictionary for the algorithm to work on, making it\n",
      "more complex to implement than stemming.\n",
      "\n",
      "Figure 23: An example of lemmatization by García et al. (2018).\n",
      "\n",
      "11.1.3 WordPiece tokenization\n",
      "\n",
      "While stemming and lemmatization gets completely rid of sufﬁxes, the WordPiece method [Wu et al. 2016] breaks\n",
      "words into smaller parts by keeping sufﬁxes using a machine learning model. In the original paper, the WorldPiece\n",
      "model is trained on a vocabulary from 8K to 32K words. The purpose of this training is to learn how to tokenize\n",
      "sentences and words efﬁciently.\n",
      "\n",
      "17\n",
      "\n",
      "\f",
      "Figure 24: WordPiece tokenization on the word \"embedding\". Illustration by McCormick (2019).\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "The \"##\" characters shown ﬁgure 24 are used to denote all WordPieces that do not start a word. Training the model to\n",
      "spilt words into WordPieces makes it able to perform on unknown data. For instance, if the model has to tokenize a\n",
      "word it hasn’t been trained on, it will try to look for roots it has studied during training and produce a segmentation\n",
      "accordingly.\n",
      "\n",
      "Figure 25: WordPiece tokenization of a word that is unknown to the model.\n",
      "\n",
      "11.1.4 N-grams\n",
      "\n",
      "Sometimes, association of words can change the meaning or bring different information. If some embedding models as\n",
      "BERT (which is described further in this paper) already take into account the context, less recent approaches require\n",
      "some processing on the words (that can be stemmed). For instance, if we are predicting the sentiment of one sentence,\n",
      "the word \"bad\" would be expected to push the output toward the negative label. However, \"not bad\" would do the\n",
      "opposite. The traditional word decomposition then need to be enriched and we might need to consider \"not bad\" as\n",
      "a unique word. The construction of new \"words\" coming from the co-occurence of words is called \"n-grams\" with\n",
      "\"n\" standing for the number of words we would like to consider (in our exemple \"not bad\" is a \"2-grams\"). When the\n",
      "embedding model cannot capture the context, this processing aims to enrich the dictionary of words.\n",
      "\n",
      "11.2 Length of the text\n",
      "\n",
      "When dealing with long texts, NLP models tend to perform less efﬁciently, as they need to process every sentences.\n",
      "However, sometimes the sentiment is not distributed throughout the whole document, but can be expressed in a single\n",
      "sentence. For instance, when working on an article, the title can convey much more information than the actual article\n",
      "with context-setting and argumentation. Moreover, the longer the text, the more bias is introduced by stopwords,\n",
      "reinforcing the need to remove them. Therefore, the length of the text is a factor to keep in mind when dealing with\n",
      "NLP tasks.\n",
      "\n",
      "18\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "12 Deeper understanding of text embedding\n",
      "\n",
      "After explaining the technical aspects of preprocessing raw words, let’s have a closer look to the methods used to make\n",
      "words understandable by a computer. The different subsections below introduce the most popular embedding techniques\n",
      "that are used in Natural Language Processing.\n",
      "\n",
      "12.1 Bag Of Words (BoW) and Term Frequency-Inverse Document Frequency (Tf-Idf)\n",
      "\n",
      "The most fundamental method of text embedding is Bag Of Word ﬁrst mentionned by Harris (1954). In order to explain\n",
      "the latter rigorously, let us deﬁne some notations.\n",
      "\n",
      "• Let c ∈ Rn be a vector of dimension n ∈ N called \"character\". It could be letter or punctuation or any unitary\n",
      "\n",
      "symbol.\n",
      "\n",
      "• We deﬁne a \"segment\" w ∈ (Rn)N as a ﬁnite sequence of \"characters\". It could be a word (if the \"characters\"\n",
      "\n",
      "are letters).\n",
      "\n",
      "• We deﬁne a \"dictionary\" as a subset D ⊂ (Rn)N\n",
      "• We deﬁne a \"document\" d ∈ DN as a ﬁnite sequence of \"segments\".\n",
      "\n",
      "d = (wi)i∈\n",
      "(cid:74)\n",
      "\n",
      "1,nd\n",
      "\n",
      "(cid:75)\n",
      "\n",
      "with nd the length of the document\n",
      "\n",
      "Given a document d, the BoW representation displays the frequency of each word in each sentence of the document.\n",
      "\n",
      "For instance, let’s study the document d = [“He is a good boy”, “She is a good girl”, “Boys and girls are good”].\n",
      "After preprocessing, d becomes d(cid:48) = [“good boy”, “good girl”, “boy girl good”]. Now let’s build a dictionary with the\n",
      "available words in d(cid:48), that we denote D. D = [\"good\", \"boy\", \"girl\"]. Using this dictionary and counting the occurrence\n",
      "of each words of the dictionary in each sentence, we get the following Bag of Words representation for the set d(cid:48):\n",
      "\n",
      "Table 1: Bag Of Words representation. Example by Naik (2020a).\n",
      "\n",
      "d(cid:48)[1] = \"good boy\"\n",
      "d(cid:48)[2] = \"good girl\"\n",
      "d(cid:48)[3] = \"boy girl good\"\n",
      "\n",
      "good\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "boy\n",
      "1\n",
      "0\n",
      "1\n",
      "\n",
      "girl\n",
      "0\n",
      "1\n",
      "1\n",
      "\n",
      "Though this representation is simple to understand, it gives an equal weight to every word and no semantic difference\n",
      "between words. However BoW can be improved by adding Term Frequency, ﬁrst mentioned by Luhn (1957) and\n",
      "Inverse Document Frequency [Sparck Jones 1972]. Let us deﬁne a word w ∈ RN, and a set d ∈ (D)N of sentences.\n",
      "Term Frequency tf (w, d) of w can be deﬁned like so :\n",
      "\n",
      "tf (w, d) = number of times the word w appears in d\n",
      "\n",
      "total number of words in d\n",
      "\n",
      "Let us denote len(d) the number of sentences in the document d and df (w) the number of sentences in d containing the\n",
      "word w. Inverse Document Frequency can be deﬁned like so :\n",
      "\n",
      "idf (w, d) = log( len(d)\n",
      "df (w) )\n",
      "\n",
      "By multiplying tf (w, d) and idf (w, d) of each words we obtain a new matrix that gives us semantic differences through\n",
      "weights that deﬁne the signiﬁcance of a word in a sentence. In the previous example, this will look like this :\n",
      "\n",
      "Table 2: tf and idf on the same example. Naik (2020b)\n",
      "\n",
      "tf\n",
      "d(cid:48)[1]\n",
      "d(cid:48)[2]\n",
      "d(cid:48)[3]\n",
      "\n",
      "good\n",
      "0.5\n",
      "0.5\n",
      "0.33\n",
      "\n",
      "boy\n",
      "0.5\n",
      "0\n",
      "0.33\n",
      "\n",
      "girl\n",
      "0\n",
      "0.5\n",
      "0.33\n",
      "\n",
      "good\n",
      "\n",
      "boy\n",
      "\n",
      "girl\n",
      "\n",
      "idf Log(3/3)=0 Log(3/2) Log(3/2)\n",
      "\n",
      "19\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Table 3: T f − Idf on the same example. Naik (2020b)\n",
      "\n",
      "Tf-Idf\n",
      "d(cid:48)[1]\n",
      "d(cid:48)[2]\n",
      "d(cid:48)[3]\n",
      "\n",
      "good\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "boy\n",
      "0.5*Log(3/2)\n",
      "0\n",
      "0.33*Log(3/2)\n",
      "\n",
      "girl\n",
      "0\n",
      "0.5*Log(3/2)\n",
      "0.33*Log(3/2)\n",
      "\n",
      "12.2 Word2Vec\n",
      "\n",
      "The BoW method presents two main disadvantages. The ﬁrst one is that it deals with very large vectors and the second\n",
      "one is that there is no notion of similarity between words, as they are treated as indices of a vocabulary set. The Tf-Idf\n",
      "adds weights to words, however there is no proper relations between words. Mikolov et al. (2013) suggested a vectorial\n",
      "representation of words that would consider the context from which they are taken, thus introducing the Continuous Bag\n",
      "Of Word (CBoW), which aim is to predict a word based on its surrounding context and the Skip-gram model, which\n",
      "aim is to predict the surrounding context based on a single word. These two models are better known as Word2Vec, as\n",
      "the original code was named.\n",
      "\n",
      "Figure 26: Architecture of the Continuous Bag of Word (left) and Skip-Gram (right) models. Illustration by Belkacem\n",
      "et al. (2017).\n",
      "\n",
      "Let us have a segment w of size N , which translates to a sentence of N words (w1, ..., wN ). We deﬁne a context\n",
      "window C of size k that can be placed anywhere in the sentence. The aim of the CBoW model is to predict the center\n",
      "word of the context window C by using the other words from the context window. This is achieved by training neural\n",
      "networks.\n",
      "\n",
      "Figure 27: Illustration of a Context Window in Continuous Bag Of Words by Padawe (2019). The aim of the training is\n",
      "to predict the red word using the blue words.\n",
      "\n",
      "The Skip-Gram algorithm works the opposite way, trying to predict a context window from the middle word (see red\n",
      "word ﬁgure 27). These two representations allow relations to be built between words.\n",
      "\n",
      "20\n",
      "\n",
      "\f",
      "After the training we can extract from the hidden layer(circled in blue ﬁgure 26) an ‘embedding layer’ or ‘embedding\n",
      "matrix’. This matrix can then be applied to any context window and similar context windows will return similar words,\n",
      "creating relations between contexts and words.\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Figure 28: Examples of relations built by a Skip-Gram model from the original paper by Mikolov et al. (2013).\n",
      "\n",
      "Figure 29: Representation of the relationship dynamics in space.\n",
      "\n",
      "12.3 Doc2Vec\n",
      "\n",
      "The Doc2Vec model is an extension of the Word2Vec model introduced by Le and Mikolov (2014). Indeed, the\n",
      "architecture is the same, but a new embedding is added, mapping a paragraph to a vectorial representation. This\n",
      "additional vector called “paragraph vector” represents the missing information from the current context. It acts like a\n",
      "memory of the topic of the paragraph.\n",
      "\n",
      "Similarly to the Word2Vec model, two representations are presented. The ﬁrst one, called Paragraph Vector with\n",
      "Distributed memory is analogous to Continuous Bag of Words, and the second one called Paragraph Vector with\n",
      "Distributed Bag Of Words is analogous to Skip-Gram.\n",
      "\n",
      "21\n",
      "\n",
      "\f",
      "Figure 30: Illustration of the Paragraph Vector with Distributed Memory (on the left) and Paragraph Vector with\n",
      "Distributed Bag of Words (on the right) by Le and Mikolov (2014).\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "12.4 From Attention to Transformers\n",
      "\n",
      "The comprehension of the following sections requires understanding of the following concepts, that won’t be detailed\n",
      "in this paper.\n",
      "\n",
      "• Recurent Neural Networks (RNN): A type of neural networks that has access to past predictions when making\n",
      "a new prediction. The information about previous outputs is conveyed through \"hidden states\". RNN are\n",
      "able to work on sequential problems, which are problems that involve sequences of different sizes, such as\n",
      "translation. An article from Simeon (2017) explains the concept of RNN with further details, and a cheat sheet\n",
      "from Afshine and Shervine summarizing the mathematical background around RNN is available on Stanford\n",
      "University’s website.\n",
      "\n",
      "• Encoder-Decoder: Also called Sequence-to-Sequence (Seq2Seq), an architecture using RNNs introduced\n",
      "by Sutskever et al.\n",
      "It performs well when the input’s and output’s lengths differ. An article\n",
      "from Kostadinov (2019) explains how the encoder-decoder architecture is built and how it allows better\n",
      "performances.\n",
      "\n",
      "(2014).\n",
      "\n",
      "One of the main problems of using a Word2Vec representation is that the relations that are built between words are\n",
      "purely statistical, and reﬂect spatial proximity only. However, the purpose of NLP is to recreate subtle and complex\n",
      "relations that are put aside by statistical representations. Moreover, with the Recurrent Neural Networks that were\n",
      "heavily used lately for NLP tasks training can be very time-consuming. In order to achieve efﬁciency, the computing\n",
      "processes involving training have to make the most of computer’s parallel behavior (using GPUs), and compute the\n",
      "highest number of operations at the same time. The attention model introduced by Vaswani et al. (2017), aims to solve\n",
      "both of these problems by recreating the subtle links that exist within a sentence, and that are caught by the human\n",
      "brain when reading, and maximizing parallel computing. The general idea is to train as well a neural network and get\n",
      "from the hidden layers a transformation model that can be used for other tasks.\n",
      "\n",
      "12.4.1 The Attention Mechanism\n",
      "\n",
      "Before the release of the Vaswani et al. (2017) paper, a common approach to capture dynamic dependencies between\n",
      "words inside a sentence was to use Recurrent Neural Networks (RNNs) which are not detailed in this document.\n",
      "The idea of using such structures is to capture temporal dependencies between words. However, RNNs have some\n",
      "drawbacks. They allocates identical importance of words independently to the output to predict. In other words, the\n",
      "structure is not able to capture context and adapt the relation (weights) between words accordingly.\n",
      "\n",
      "Attention models are neural networks layers that have been introduced to overcome that limitation. When looking at\n",
      "an input sequence, Attention is the mechanism that draws the relationships between the elements of the sequence.\n",
      "In other words, this mechanism emphasizes how every part of the input interact between themselves and how much\n",
      "inﬂuence one part has on another one. These interactions, which reproduce the context of the input, are represented in\n",
      "an Attention Matrix.\n",
      "\n",
      "22\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "In other words, as mentionned by Mohammed Terry-Jack12, \"an attention mechanism calculates the dynamic (alignment)\n",
      "weights representing the relative importance of the inputs in the sequence (the keys noted K) for that particular output\n",
      "(the query noted Q). Multiplying the dynamic weights (the alignment scores) with the input sequence (the values noted\n",
      "V ) will then weight the sequence\".\n",
      "\n",
      "Figure 31: Illustration of the mechanism applied to a simple sentence by Halthor (2020).\n",
      "\n",
      "In the original paper [Vaswani et al. 2017], two forms of Attention (so way to compute dynamics weights) are\n",
      "introduced. The ﬁrst one is called \"Scaled Dot-Product Attention\" which is the application of matrix multiplications on\n",
      "context-based matrix. This operation outputs a new matrix giving information on inner interactions within the inputs.\n",
      "The second one called \"Multi-Head Attention\" is the concatenation of several Attention Matrices calculated on the\n",
      "same input, but with different parameters. This allows to have several representations of an Attention Matrix, which are\n",
      "then averaged, in order to reduce the risk of having a low-performance attention mechanism that would focus too much\n",
      "on the wrong parts of the input.\n",
      "\n",
      "Figure 32: Architecture of the two attention mechanism as presented in the original paper[Vaswani et al. 2017]. Q, K\n",
      "and V are matrices that are components of the input.\n",
      "\n",
      "Let’s introduce some notations before going into further explanations. We will denote matrices dimension Rn×p instead\n",
      "of M n×p(R). As the crux of the matter in this paper is Natural Language Processing, we will assume the inputs we\n",
      "\n",
      "12https://medium.com/@b.terryjack/deep-learning-the-transformer-9ae5e9c5a190\n",
      "\n",
      "23\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "are working with are textual. Let us denote a pre-processed input matrix X ∈ Rn×p with p ∈ N the dimension of the\n",
      "embeddings, which is also the uniform size of all segment vectors (see deﬁnition of a segment section 12.2 page 20)\n",
      "and n ∈ N the length of the input sequence. X has already gone through input embedding and positional embedding.\n",
      "\n",
      "12.4.2 Scaled Dot-Product Attention\n",
      "\n",
      "Before tackling Multi-Head Attention, let’s see how the input matrix is being processed with Scaled Dot-Product\n",
      "Attention (number of heads h = 1), by paying close attention to the dimensions of each matrix.\n",
      "Let W Q ∈ Rp×dk , W K ∈ Rp×dk , W V ∈ Rp×dv , be three weight matrices from which we can obtain the following Q,\n",
      "K and V 13 matrices :\n",
      "\n",
      "Q = X · W Q ∈ Rn×dk\n",
      "K = X · W K ∈ Rn×dk\n",
      "V = X · W V ∈ Rn×dv\n",
      "\n",
      "Figure 33: Illustration by Allamar (2018)\n",
      "\n",
      "Initially the values of the weight matrices are arbitrary, and will be adjusted later using a Feed-Forward Neural Network\n",
      "(see section 12.4.5 page 26). We now calculate the Attention using the formula from the original paper :\n",
      "\n",
      "Attention(Q, K, V ) = softmax( Q·KT\n",
      "dk\n",
      "\n",
      "√\n",
      "\n",
      ") · V ∈ Rn×dv\n",
      "\n",
      "Once the Attention matrix is calculated, it is multiplied by a matrix W 0 ∈ Rdv×p, so that the ﬁnal output is a matrix :\n",
      "\n",
      "12.4.3 Multi-Head Attention\n",
      "\n",
      "Z ∈ Rn×p\n",
      "\n",
      "As it was brieﬂy explained before, Multi-Head Attention is the concatenation of several Attention matrices calculated\n",
      "on the same input. Therefore the notations are the same as in the previous section, with an input matrix X ∈ Rn×p, but\n",
      "this time, the number of heads h > 1. In the original paper, the actual value of h is 8.\n",
      "\n",
      "13Q,K and V stand for Query, Key and Value. They are parameters matrices that will be modiﬁed as the weight matrices will be\n",
      "\n",
      "adjusted during training. For more information about what each matrix represents, reader may refer to [Terry-Jack 2019].\n",
      "\n",
      "24\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "In the previous part, only 3 weight matrices W Q, W K, W V were deﬁned. However, Multi-Head Attention requires\n",
      "to deﬁne those three matrices for each head. Therefore, for i ∈ {0, .., h} we will have W Q\n",
      "i ∈\n",
      "Rp×dk , W V\n",
      "\n",
      "i ∈ Rp×dk , W K\n",
      "\n",
      "i ∈ Rp×dv , with dk = dv = p\n",
      "h .\n",
      "\n",
      "We then calculate Qi ∈ Rn×dk , Ki ∈ Rn×dk , Vi ∈ Rn×dv and Zi =Attention(Qi, Ki, Vi) ∈ Rn×dv for each head as\n",
      "done previously.\n",
      "Finally, the Zi are concatenated, giving a matrix Concat(Zi) ∈ Rn×(dv∗h), and the concatenation is multiplied by\n",
      "W 0 ∈ R(dv∗h)×p in order to output the ﬁnal matrix :\n",
      "\n",
      "Z ∈ Rn×p\n",
      "\n",
      "Figure 34: Step by step illustration of Multi-Head Attention by Allamar (2018).\n",
      "\n",
      "12.4.4 Transformers Neural Network\n",
      "\n",
      "The Attention model was combined with neural networks in order to build a new type of model that essentially relies\n",
      "on attention: The Transformer Neural Network. This model is based on an encoder-decoder architecture. Attention\n",
      "is used in the encoder part, the decoder part and the encoder-decoder part. This new type of model allows working\n",
      "on sequential problems without resorting to Recurrent Neural Networks (RNN), as Feed-Forward Neural Networks\n",
      "(FFN) are used at the top of both the encoder and the decoder. Using Feed-Forward Neural Networks instead of RNN\n",
      "improves calculation time and thus efﬁciency, allowing deeper and sturdier models to be trained using this technology.\n",
      "\n",
      "25\n",
      "\n",
      "\f",
      "Figure 35: Architecture of the Transformer model from the original paper of Vaswani et al. (2017).\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Before being fed to the Multi-Head Attention, the inputs go through two embeddings. The ﬁrst one is a classic input\n",
      "embedding using one of the many methods available, like those listed in subsection 12 for text data. The second\n",
      "embedding is a Positional Encoding. This type of encoding aims to implement the notion of relative positions of the\n",
      "sequence (or time signal), like the order of words in a sentence, by indicating a numeric representation of a word’s\n",
      "position in a sentence. Unlike RNNs, Feed-Forward Neural Networks won’t have access to this information by their\n",
      "own, hence the need of positional encoding.\n",
      "\n",
      "12.4.5 Feed-Forward Neural Network (FFN)\n",
      "\n",
      "As shown on Figure 35, the output of the Multi-Head Attention will be passed through a Feed-Forward neural network\n",
      "of the form :\n",
      "\n",
      "FFN(x) =max(0, xW 1 + b1)W 2 + b2\n",
      "\n",
      "By back-propagation, training FFN(C) will have the effect of adjusting the values of every W Q\n",
      "needed for the calculation of Z, thus teaching attention to the model.\n",
      "Eventually, the identiﬁed parameters for calibration are W Q\n",
      "i . These parameters will be adjusted throughout\n",
      "the learning process of Transformers Neural Networks, and once it’s done, we extract an operational attention model\n",
      "from the hidden layers.\n",
      "\n",
      "i matrices\n",
      "\n",
      "i , W K\n",
      "i\n",
      "\n",
      "i , W K\n",
      "i\n",
      "\n",
      ", W V\n",
      "\n",
      ", W V\n",
      "\n",
      "12.5 Bidirectional Encoder Representation for Transformers (BERT)\n",
      "\n",
      "BERT is one of the most recent models developed by Jacob et al. (2018) for encoding inputs before addressing NLP\n",
      "tasks. The two main steps of training a BERT model are pre-training and ﬁne-tuning, using Transformers Neural\n",
      "Networks. Pre-training is the part where the model learns about the language and how context operates in the said\n",
      "language, while ﬁne-tuning addresses the speciﬁc issue to which the model is deployed. For comprehension purposes,\n",
      "let us assume the task that is being studied is tweet sentiment analysis.\n",
      "\n",
      "26\n",
      "\n",
      "\f",
      "Figure 36: Architecture of a BERT model with the two training phases illustrated by Jacob et al. (2018)\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "In order to deal with theses two tasks, the BERT model’s architecture is made of a series of encoders from the\n",
      "Transformers neural network architecture. In the previous paragraph, we identiﬁed W Q\n",
      "i as the weight\n",
      "parameters to calibrate during training. As BERT’s architecture is a succession of several encoders, there will be as\n",
      "many W Q\n",
      "i matrices to calibrates as there are stacked encoders. Each of these matrices will be adjusted in a\n",
      "way that allows the model to understand the language it is trained on.\n",
      "\n",
      "i , W K\n",
      "i\n",
      "\n",
      "i , W K\n",
      "i\n",
      "\n",
      ", W V\n",
      "\n",
      ", W V\n",
      "\n",
      "Figure 37: Stacking the encoder part from Figure 35 to build a BERT model. Illustration by Halthor (2020).\n",
      "\n",
      "12.5.1 Inputs Representation\n",
      "\n",
      "Before going through pre-training and ﬁne-tuning, let’s see how the inputs are treated. In order to be processed by the\n",
      "model, the inputs are fed by pair of sentences (A, B) in which the words are tokenized using a WordPiece embedding\n",
      "(see section 11.1.3 page 17). Moreover, each sequence starts with a special classiﬁcation token ([CLS]) and sentences\n",
      "are separated by a separation token ([SEP ]).\n",
      "\n",
      "In order to be processed by the model, a succession of embeddings is applied to the input (the tweets).\n",
      "\n",
      "• First, a special classiﬁcation token ([CLS]) is added to the beginning of each sequence and sentences are\n",
      "\n",
      "separated with a separation token ([SEP]).\n",
      "\n",
      "• Then the inputs are tokenized using a WordPiece embedding (Wu et al. (2016)), based on a vocabulary of\n",
      "30,000 words. This type of tokenization splits a word int sub-words allowing words with similar sub-words to\n",
      "be attributed close semantic relations.\n",
      "\n",
      "• A segment embedding is also applied to each token, denoting which sentence the word belongs to in the text.\n",
      "\n",
      "• Finally, a positional embedding is applied, representing the location of the word in the text.\n",
      "\n",
      "All of these operations combined provide the embedding applied to an input before being fed to the BERT model.\n",
      "\n",
      "27\n",
      "\n",
      "\f",
      "Figure 38: BERT input representation from the original paper [Jacob et al. 2018].\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "12.5.2 Pre-training\n",
      "\n",
      "As the name itself suggests it, BERT is a combination of several encoders stacked together, extracted from Transformers\n",
      "neural networks.\n",
      "\n",
      "This bidirectional encoder architecture allows the pre-training phase to take over two main tasks: Masked Language\n",
      "Modeling (MLM) and Next Sentence Prediction (NSP). During MLM, a sentence d = (w1, ..., wn) is passed, where\n",
      "wi ∈ RN, i ∈\n",
      "are words. Approximately 15% 14 of the words in each sequence are replaced with a [M ASK]\n",
      "token , and the model is trained to guess the masked tokens. The output vector (O1, ..., On) is then passed through\n",
      "a classiﬁcation layer and eventually a softmax layer to obtain a distribution over the available vocabulary, indicating\n",
      "which word is more likely to be hidden behind the [M ASK] token.\n",
      "\n",
      "1, n\n",
      "(cid:75)\n",
      "\n",
      "(cid:74)\n",
      "\n",
      "Figure 39: Masked Language Modeling with a GELU activation function illustrated by Horev (2018).\n",
      "\n",
      "The NSP part of pre-training is a binary classiﬁcation problem. In this part, given two sentences A and B, the model\n",
      "has to guess if sentence B follows sentence A or not. The output of the [CLS] token will be passed through a binary\n",
      "classiﬁcation layer and a softmax layer, similarly to MLM, and a vector C of shape 2 × 1 will display the probability of\n",
      "the two sentences coming in succession.\n",
      "\n",
      "14The masking conditions are in reality a bit more complex than just hiding 15% of the words. See the original article [Jacob et al.\n",
      "\n",
      "2018] for more details.\n",
      "\n",
      "28\n",
      "\n",
      "\f",
      "Figure 40: Next Sentence Prediction task illustrated by Jacob et al. (2018)\n",
      "\n",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "These two tasks are trained simultaneously, the aim of the pre-training phase being the minimization of the combined\n",
      "loss function of Masked Language Modeling and Next Sentence Prediction.\n",
      "\n",
      "Text passages from BooksCorpus (800M words) [Zhu et al. 2015] and English Wikipedia (2,500M words) are used to\n",
      "train the model on MLM and NSP.\n",
      "\n",
      "12.5.3 Fine-Tuning\n",
      "\n",
      "The ﬁne-tuning phase on the other hand is quite easy to understand. It is the moment BERT uses the learnt language\n",
      "mechanics to solve a speciﬁc task. Just like in transfer learning, the input and output layer speciﬁc to our sentiment\n",
      "analysis task are added to the BERT model, and the parameters are adjusted accordingly. In our pre-training phase, the\n",
      "inputs are fed two by two (A, B). However, as the ﬁne-tuning phase is task-speciﬁc, in our case the inputs will be a\n",
      "pair (A, B) where A = a tweet and B = ∅. For the outputs, a classiﬁcation layer that will deliver the sentiment of the\n",
      "tweet is added at the end of the model. The [CLS] representation will be fed to this new output layer and used for the\n",
      "sentiment prediction.\n",
      "\n",
      "12.5.4 Review of the BERT model\n",
      "\n",
      "In the original paper, Jacob et al. (2018) present two BERT models of different sizes : the base one, stacking 12\n",
      "encoders from the Transformers representation, with a total of 110M parameters and the large one, stacking 24 encoders,\n",
      "with a total of 340M parameters. As the number of parameters of both models suggests, BERTs are quite brute force,\n",
      "but when compared to their competitors on tasks such as Sentence Classiﬁcation or Question Answering, BERT models\n",
      "are the most efﬁcient today.\n",
      "\n",
      "Although they achieve better results than other popular models on NLP tasks, they still take long to ﬁne-tune, even\n",
      "when the uploaded models are already pre-trained. From it release in 2019, BERT has been extended and improved for\n",
      "many tasks. However, usages are limited by memory since new text representation models are more and more greedy.\n",
      "\n",
      "13 Reminder on machine learning metrics that can be used also for text mining\n",
      "\n",
      "Model evaluation metrics are required to quantify model performance.\n",
      "\n",
      "13.1 Classiﬁcation Metrics\n",
      "\n",
      "To evaluate the performance of classiﬁcation models, there are several possible metrics. Here, we deﬁne a few indicators\n",
      "in a non-exhaustive way. Some common terms to be clear with are:\n",
      "\n",
      "• True positives (TP) Predicted positive and are actually positive.\n",
      "• False positives (FP) Predicted positive and are actually negative.\n",
      "• True negatives (TN) Predicted negative and are actually negative.\n",
      "\n",
      "29\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "• False negatives (FN) Predicted negative and are actually positive.\n",
      "\n",
      "Table 4: Metrics for classiﬁcation\n",
      "\n",
      "Metrics\n",
      "\n",
      "Confusion matrix\n",
      "\n",
      "Formula\n",
      "(cid:20)T P F N\n",
      "F P T N\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "Accuracy\n",
      "\n",
      "T P +T N\n",
      "T P +T N +F P +F N\n",
      "\n",
      "Precision\n",
      "\n",
      "Recall\n",
      "\n",
      "Speciﬁcity\n",
      "\n",
      "T P\n",
      "T P +F P\n",
      "\n",
      "T P\n",
      "T P +F N\n",
      "\n",
      "T N\n",
      "T N +F P\n",
      "\n",
      "F-score\n",
      "\n",
      "(1+β2)T P\n",
      "(1+β2)T P +β2F N +F P\n",
      "\n",
      "ROC\n",
      "\n",
      "AUC\n",
      "\n",
      "Meaning\n",
      "\n",
      "Range\n",
      "\n",
      "A confusion matrix is an N×N\n",
      "matrix, where N is the number\n",
      "of classes being predicted.\n",
      "It\n",
      "gives the counts of correct and\n",
      "incorrect classiﬁcations for each\n",
      "class.\n",
      "\n",
      "of positive\n",
      "\n",
      "Percentage of correctly classi-\n",
      "ﬁed instances out of the total pre-\n",
      "dicted instances.\n",
      "in-\n",
      "Percentage\n",
      "stances out of the total predicted\n",
      "positive instances.\n",
      "Percentage of positive instances\n",
      "out of the total actual positive\n",
      "instances.\n",
      "Percentage of negative instances\n",
      "out of the total actual negative\n",
      "instances.\n",
      "β is a positive real, and it is cho-\n",
      "sen such that recall is considered\n",
      "β times as important as precision.\n",
      "In practice β is often set to 1, F1-\n",
      "score is the harmonic mean of\n",
      "precision and recall.\n",
      "The Receiver Operator Char-\n",
      "acteristic curve represents the\n",
      "tradeoff between Recall and\n",
      "Speciﬁcity.\n",
      "AUC is the area under the ROC\n",
      "curve.\n",
      "\n",
      "The higher the diagonal\n",
      "values of the confusion\n",
      "matrix the better. Con-\n",
      "versely, the lower the val-\n",
      "ues off the diagonal the\n",
      "better. All values are pos-\n",
      "itive reals.\n",
      "The accuracy is between\n",
      "0 and 1, where 1 is the\n",
      "perfect score.\n",
      "As above.\n",
      "\n",
      "As above.\n",
      "\n",
      "As above.\n",
      "\n",
      "As above.\n",
      "\n",
      "The closer a curve is to\n",
      "the top left corner the bet-\n",
      "ter.\n",
      "\n",
      "The AUC value lies be-\n",
      "tween 0 and 1 where 1 in-\n",
      "dicates an excellent clas-\n",
      "siﬁer and 0.5 the random\n",
      "model.\n",
      "\n",
      "These metrics are valid for binary classiﬁcation, for multi-label classiﬁcation we can use micro, macro or samples\n",
      "averages.\n",
      "\n",
      "The micro average allows to calculate the metric with TP, TN, FP, FN of the k-classes, for example for the precision :\n",
      "\n",
      "P REmicro =\n",
      "\n",
      "T P1 + ... + T Pk\n",
      "T P1 + ... + T Pk + F P1 + ... + F Pk\n",
      "\n",
      "The macro average is obtained by averaging the metric of each class:\n",
      "\n",
      "P REmacro =\n",
      "\n",
      "P RE1 + ... + P REk\n",
      "k\n",
      "\n",
      "The samples average returns the average metric of each instance.\n",
      "\n",
      "All these metrics need to be compared with a baseline model. A great score doesn’t mean that the model performs well\n",
      "if a random model has also a good score. A straightforward baseline model is the dummy classiﬁer that always predict\n",
      "the same result.\n",
      "\n",
      "30\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "13.2 Regression Metrics\n",
      "\n",
      "The regression task, unlike the classiﬁcation task, outputs continuous value within a given range.\n",
      "\n",
      "Table 5: Metrics for Regression\n",
      "\n",
      "Metrics\n",
      "\n",
      "Formula\n",
      "\n",
      "Meaning\n",
      "\n",
      "MAE\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "(cid:80)N\n",
      "\n",
      "i=1 |yi − ˆyi|\n",
      "\n",
      "MSE\n",
      "\n",
      "1\n",
      "N\n",
      "\n",
      "(cid:80)N\n",
      "\n",
      "i=1(yi − ˆyi)2\n",
      "\n",
      "RMSE\n",
      "\n",
      "(cid:113) (cid:80)N\n",
      "\n",
      "i=1(yi−ˆyi)2\n",
      "N\n",
      "\n",
      "The Mean Absolute Error is the average of the dif-\n",
      "ference between the actual values and the predicted\n",
      "values.\n",
      "\n",
      "The Mean Squared Error measures the square of\n",
      "the difference between the actual values and the\n",
      "predicted values.\n",
      "\n",
      "The Root Mean Squared Error measures the aver-\n",
      "age magnitude of the error by taking the square\n",
      "root of the average of squared differences between\n",
      "prediction and actual observation.\n",
      "\n",
      "Range\n",
      "\n",
      "[0; +∞[\n",
      "\n",
      "[0; +∞[\n",
      "\n",
      "[0; +∞[\n",
      "\n",
      "R2\n",
      "\n",
      "1 − M SE(model)\n",
      "M SE(baseline)\n",
      "\n",
      "The Coefﬁcient of Determination or R2 compares\n",
      "the current model with a constant baseline.\n",
      "\n",
      "] − ∞; 1]\n",
      "\n",
      "13.3 Business Metrics\n",
      "\n",
      "In addition to statistical metrics, model ﬁtting and selection must be completed by business assessment. It is hard to\n",
      "deﬁne exhaustive list of business metrics but as an example, using NLP for claims adjudications has also been assessed\n",
      "in term of losses for claims paid that should not have been versus, claims we can decline more efﬁciently thanks to\n",
      "scoring. Projecting algorithms decision into business metrics (like estimated loss ratio or A/E ratio in insurance) is\n",
      "key. It is worth to inform reader about it since to bring value, text mining algorithms should not only be integrated in\n",
      "technological solutions but need also to be calibrated to optimize an operational metric.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "This survey presented different processes and techniques that are common in Natural Language Processing. Research in\n",
      "this area has been tremendous since the past ten years. BERT seems today to delimit a new era in the usage of textual\n",
      "information. The frequency of new models releases lead to continuously adapt text mining algorithms but also provide\n",
      "a large amount of new opportunities in the insurance industry.\n",
      "\n",
      "31\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "References\n",
      "\n",
      "A. Afshine and A. Shervine. Recurrent neural networks cheatsheet. Stanford. URL https://stanford.edu/\n",
      "\n",
      "~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks.\n",
      "\n",
      "C. C. Aggarwal and C. X. Zhai. A survey of text clustering algorithms. Text Mining Data, Springer, pages 77–121,\n",
      "\n",
      "2012a.\n",
      "\n",
      "Charu C. Aggarwal and ChengXiang Zhai. An Introduction to Text Mining, pages 1–10. Springer US, Boston, MA,\n",
      "ISBN 978-1-4614-3223-4. doi: 10.1007/978-1-4614-3223-4_1. URL https://doi.org/10.1007/\n",
      "\n",
      "2012b.\n",
      "978-1-4614-3223-4_1.\n",
      "\n",
      "J. Allamar.\n",
      "\n",
      "The illustrated transformer.\n",
      "\n",
      "github, 2018.\n",
      "\n",
      "URL http://jalammar.github.io/\n",
      "\n",
      "illustrated-transformer/.\n",
      "\n",
      "M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web.\n",
      "\n",
      "20th Joint International Conference On Artiﬁcial Intelligence, pages 2670–2676, 2007.\n",
      "\n",
      "T. Belkacem, T. Dkaki, J. Moreno, and — Mohand. Apprentissage de représentations de documents et leur exploitation\n",
      "\n",
      "en recherche d’information. 03 2017.\n",
      "\n",
      "D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. Nymble: a high-performance learning name-ﬁnder. 5th\n",
      "\n",
      "Conference on Applied Natural Language Processing, page 194–201, 1997.\n",
      "\n",
      "M. Le Blanc and J. Crowley. Relative risk trees for censored survival data. Biometrics, 1992.\n",
      "\n",
      "M. E. Califf and R. J. Mooney. Relational learning of pattern-match rules for information extraction. 16th National\n",
      "Conference on Artiﬁcial Intelligence and the 11th Innovative Applications of Artiﬁcial Intelligence Conference, page\n",
      "328–334, 1999.\n",
      "\n",
      "A. Charpentier, E. Flachaire, and A. Ly. Econometrics and machine learning. 2018. ISSN 0336-1454. doi: 10.24187/\n",
      "ecostat.2018.505d.1970. URL https://www.persee.fr/doc/estat_0336-1454_2018_num_505_1_10873.\n",
      "Gerald DeJong. Prediction and substantiation: A new approach to natural language processing. Cognitive Science,\n",
      "ISSN 0364-0213. doi: https://doi.org/10.1016/S0364-0213(79)80009-9. URL http:\n",
      "\n",
      "3(3):251 – 273, 1979.\n",
      "//www.sciencedirect.com/science/article/pii/S0364021379800099.\n",
      "\n",
      "D. Delcaillau, A. Ly, F. Vermet, and A. Papp. Interpretabilité des modèles : état des lieux des méthodes et application à\n",
      "\n",
      "l’assurance [translation in progress], 2020.\n",
      "\n",
      "M. d’Hoffschmidt, W. Belblidia, T . Brendlé, Q. Heinrich, and M. Vidal. Fquad: French question answering dataset,\n",
      "\n",
      "2020.\n",
      "\n",
      "K. Ganesan. All you need to know about text preprocessing for nlp and machine learning. KDnuggets, 2019. URL\n",
      "\n",
      "https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html.\n",
      "\n",
      "C. García,\n",
      "stemming\n",
      "what-is-the-difference-between-stemming-and-lemmatization/.\n",
      "\n",
      "and B. Ramirez.\n",
      "bitext,\n",
      "\n",
      "Pedro Cabanilles,\n",
      "lemmatization?\n",
      "\n",
      "J.\n",
      "and\n",
      "\n",
      "2018.\n",
      "\n",
      "What\n",
      "\n",
      "URL\n",
      "\n",
      "is\n",
      "\n",
      "the\n",
      "\n",
      "between\n",
      "https://blog.bitext.com/\n",
      "\n",
      "difference\n",
      "\n",
      "J. Gareth, D. Witten, T. Hastie, and R. Tibshirani. An introduction to statistical learning : with applications in R, page\n",
      "\n",
      "vii. New York : Springer, 2013. Includes bibliographical references and index.\n",
      "\n",
      "Y. Goldberg and O. Levy. word2vec explained: deriving mikolov et al.’s negative-sampling word-embedding method.\n",
      "\n",
      "arXiv preprint arXiv:1402.3722, 2014.\n",
      "\n",
      "Ralph Grishman and Beth Sundheim. Message understanding conference- 6: A brief history. In COLING 1996 Volume\n",
      "1: The 16th International Conference on Computational Linguistics, 1996. URL https://www.aclweb.org/\n",
      "anthology/C96-1079.\n",
      "\n",
      "S. Gupta, A. Nenkova, and D. Jurafsky. Measuring importance and query relevance in topic-focused multi-document\n",
      "\n",
      "summarization. Association for Computational Linguistics, pages 193–196, 2007.\n",
      "\n",
      "Ishwara H., Kogalur U. B., Blackstone E. H., and Lauer M. S. Random survival forests. The Annals of Applied Statistics,\n",
      "\n",
      "2008.\n",
      "\n",
      "A. Halthor. Bert neural network - explained!, 2020. URL https://www.youtube.com/watch?edufilter=NULL&\n",
      "\n",
      "v=xI0HHN5XKDo.\n",
      "\n",
      "Zellig S. Harris. Distributional structure. <i>WORD</i>, 10(2-3):146–162, 1954. doi: 10.1080/00437956.1954.\n",
      "\n",
      "11659520. URL https://doi.org/10.1080/00437956.1954.11659520.\n",
      "\n",
      "R. Hooper and C. Paice. The lancaster stemming algorithm. Program, 2005.\n",
      "\n",
      "32\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "R. Horev.\n",
      "wards\n",
      "bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270.\n",
      "\n",
      "language model\n",
      "\n",
      "art\n",
      "URL\n",
      "\n",
      "explained:\n",
      "\n",
      "Science,\n",
      "\n",
      "2018.\n",
      "\n",
      "State\n",
      "\n",
      "Data\n",
      "\n",
      "Bert\n",
      "\n",
      "the\n",
      "\n",
      "To-\n",
      "for\n",
      "https://towardsdatascience.com/\n",
      "\n",
      "nlp.\n",
      "\n",
      "of\n",
      "\n",
      "D. Jacob, C. Ming-Wei, L. Kenton, and T. . Bert: Pre-training of deep bidirectional transformers for language\n",
      "\n",
      "understanding, 2018.\n",
      "\n",
      "J. Jiang. Information Extraction from Text, pages 11–41. 2012.\n",
      "J. Jiang and C. Zhai. A systematic exploration of the feature space for relation extraction. Language Technology\n",
      "Conference of the North American Chapter of the Association for Computational Linguistics, page 113–120, 2007.\n",
      "D. Jurafsky. Text classiﬁcation and naïve bayes. Stanford, 2011. URL https://web.stanford.edu/class/cs124/\n",
      "\n",
      "lec/naivebayes.pdf.\n",
      "\n",
      "S. Kostadinov.\n",
      "\n",
      "Understanding\n",
      "\n",
      "encoder-decoder\n",
      "\n",
      "wards\n",
      "understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346.\n",
      "\n",
      "Science,\n",
      "\n",
      "2019.\n",
      "\n",
      "Data\n",
      "\n",
      "sequence\n",
      "URL\n",
      "\n",
      "to\n",
      "\n",
      "sequence model.\n",
      "\n",
      "To-\n",
      "https://towardsdatascience.com/\n",
      "\n",
      "Quoc V. Le and Tomas Mikolov. Distributed representations of sentences and documents, 2014.\n",
      "H. P. Luhn. A statistical approach to mechanized encoding and searching of literary information. IBM Journal of\n",
      "\n",
      "Research and Development, 1(4):309–317, 1957.\n",
      "\n",
      "H. P. Luhn. Key word-in-context index for technical literature (kwic index). American Documentation, 11(4):288–\n",
      "295, 1960. doi: 10.1002/asi.5090110403. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.\n",
      "5090110403.\n",
      "\n",
      "S. M Lundberg, B. Nair, M. S Vavilala, M. Horibe, M. J Eisses, T. Adams, D. E Liston, D. King-Wai Low, S. Newman,\n",
      "J. Kim, et al. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nature\n",
      "biomedical engineering, 2(10):749–760, 2018.\n",
      "\n",
      "A. Ly. Machine learning algorithms in insurance : solvency, textmining, anonymization and transparency. Theses,\n",
      "\n",
      "Université Paris-Est, November 2019. URL https://tel.archives-ouvertes.fr/tel-02413664.\n",
      "\n",
      "C. McCormick. Bert research - ep. 2 - wordpiece embeddings, 2019. URL https://www.youtube.com/watch?\n",
      "\n",
      "edufilter=NULL&v=zJW57aCBCTk.\n",
      "\n",
      "M.F.Porter. An algorithm for sufﬁx stripping. Program, 14(3):130–137, Jan 1980. ISSN 0033-0337. doi: 10.1108/\n",
      "\n",
      "eb046814. URL https://doi.org/10.1108/eb046814.\n",
      "\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector\n",
      "\n",
      "space, 2013.\n",
      "\n",
      "K. Naik. Natural language processing|bag of words intuition, 2020a. URL https://www.youtube.com/watch?\n",
      "\n",
      "edufilter=NULL&v=IKgBLTeQQL8.\n",
      "\n",
      "K. Naik. Natural language processing|tf-idf intuition| text prerocessing, 2020b. URL https://www.youtube.com/\n",
      "\n",
      "watch?edufilter=NULL&v=D2V1okCEsiE.\n",
      "\n",
      "A. Nenkova and K. McKeown. A Survey of Text Summarization Techniques. 2012.\n",
      "G. Padawe. Word2vector using gensim. Medium, 2019. URL https://medium.com/analytics-vidhya/\n",
      "\n",
      "word2vector-using-gensim-e055d35f1cb4.\n",
      "\n",
      "J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014\n",
      "\n",
      "conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014.\n",
      "\n",
      "L. R. Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. 77:257–286, 1989.\n",
      "L. F. Rau. Extracting company names from text. 7th IEEE Conference on Artiﬁcial Intelligence Application, page\n",
      "\n",
      "29–32, 1991.\n",
      "\n",
      "M. Schuster and K. Nakajima. Japanese and korean voice search. In 2012 IEEE International Conference on Acoustics,\n",
      "\n",
      "Speech and Signal Processing (ICASSP), pages 5149–5152. IEEE, 2012.\n",
      "\n",
      "K. Simeon. How recurrent neural networks work. Towards Data Science, 2017. URL https://towardsdatascience.\n",
      "\n",
      "com/learn-how-recurrent-neural-networks-work-84e975feaaf7.\n",
      "\n",
      "K. Sparck Jones. A statistical interpretation of term speciﬁcity and its application in retrieval. Journal of Documentation,\n",
      "28(1):11–21, Jan 1972. ISSN 0022-0418. doi: 10.1108/eb026526. URL https://doi.org/10.1108/eb026526.\n",
      "\n",
      "R. Speer, J. Chin, and C. Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. 2017. URL\n",
      "\n",
      "http://arxiv.org/abs/1612.03975.\n",
      "\n",
      "33\n",
      "\n",
      "\f",
      "A PREPRINT - OCTOBER 2, 2020\n",
      "\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks, 2014.\n",
      "M. Terry-Jack. Deep learning: The transformer. Medium, 2019. URL https://medium.com/@b.terryjack/\n",
      "\n",
      "deep-learning-the-transformer-9ae5e9c5a190.\n",
      "\n",
      "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, Aidan N. Gomez, L. Kaiser, and I. Polosukhin. Attention is\n",
      "\n",
      "all you need, 2017.\n",
      "\n",
      "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun,\n",
      "Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser,\n",
      "Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei\n",
      "Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and\n",
      "Jeffrey Dean. Google’s neural machine translation system: Bridging the gap between human and machine translation,\n",
      "2016.\n",
      "\n",
      "Liu Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A\n",
      "\n",
      "robustly optimized bert pretraining approach, 2019.\n",
      "\n",
      "D. Zelenko, A. Chinatsu, and A. Richardella. Kernel methods for relation extraction. Journal of Machine Learning\n",
      "\n",
      "Research, 3:1083–1106, 2003.\n",
      "\n",
      "M. Zhang, J. Zhang, and J. Su. Exploring syntactic features for relation extraction using a convolution tree kernel.\n",
      "Human Language Technology Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics, page 288–295, 2006.\n",
      "\n",
      "Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies:\n",
      "\n",
      "Towards story-like visual explanations by watching movies and reading books, 2015.\n",
      "\n",
      "34\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4c29da-f60b-4da0-84b9-93530ceda0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "#Tokenize the text reader from PDF\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "\n",
    "#Find the frequence distribution\n",
    "freqdist=FreqDist(tokens)\n",
    "long_freq_words=[words for words in tokens if len(words)>10 and freqdist[words]>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183d7961-3ce4-4847-8854-9aa0150f816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREPRINT',\n",
       " 'different',\n",
       " 'insurance',\n",
       " 'language',\n",
       " 'insurance',\n",
       " 'information',\n",
       " 'analysis',\n",
       " 'mining',\n",
       " 'insurance',\n",
       " 'insurance',\n",
       " 'different',\n",
       " 'mining',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'insurance',\n",
       " 'mining',\n",
       " 'analysis',\n",
       " 'sentence',\n",
       " 'speciﬁc',\n",
       " 'models',\n",
       " 'different',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'information',\n",
       " 'process',\n",
       " 'mining',\n",
       " 'between',\n",
       " 'mining',\n",
       " 'language',\n",
       " 'information',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'Figure',\n",
       " 'Figure',\n",
       " 'insurance',\n",
       " 'insurance',\n",
       " 'different',\n",
       " 'insurance',\n",
       " 'analysis',\n",
       " 'speciﬁc',\n",
       " 'process',\n",
       " 'different',\n",
       " 'analysis',\n",
       " 'models',\n",
       " 'insurance',\n",
       " 'mining',\n",
       " 'process',\n",
       " 'insurance',\n",
       " 'analysis',\n",
       " 'information',\n",
       " 'process',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'process',\n",
       " 'called',\n",
       " 'analysis',\n",
       " 'claims',\n",
       " 'classiﬁcation',\n",
       " 'claims',\n",
       " 'process',\n",
       " 'mining',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'insurance',\n",
       " 'claims',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'speciﬁc',\n",
       " 'different',\n",
       " 'models',\n",
       " 'speciﬁc',\n",
       " 'different',\n",
       " 'models',\n",
       " 'insurance',\n",
       " 'different',\n",
       " 'different',\n",
       " 'models',\n",
       " 'insurance',\n",
       " 'process',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'process',\n",
       " 'different',\n",
       " 'models',\n",
       " 'information',\n",
       " 'different',\n",
       " 'information',\n",
       " 'information',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'information',\n",
       " 'models',\n",
       " 'information',\n",
       " 'information',\n",
       " 'information',\n",
       " 'between',\n",
       " 'information',\n",
       " 'process',\n",
       " 'information',\n",
       " 'information',\n",
       " 'process',\n",
       " 'context',\n",
       " 'speciﬁc',\n",
       " 'context',\n",
       " 'information',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'representation',\n",
       " 'language',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'learning',\n",
       " 'classiﬁcation',\n",
       " 'claims',\n",
       " 'models',\n",
       " 'insurance',\n",
       " 'classiﬁcation',\n",
       " 'models',\n",
       " 'analysis',\n",
       " 'claims',\n",
       " 'classiﬁcation',\n",
       " 'process',\n",
       " 'Figure',\n",
       " 'process',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'Figure',\n",
       " 'classiﬁcation',\n",
       " 'models',\n",
       " 'insurance',\n",
       " 'claims',\n",
       " 'process',\n",
       " 'claims',\n",
       " 'insurance',\n",
       " 'insurance',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'models',\n",
       " 'models',\n",
       " 'Figure',\n",
       " 'claims',\n",
       " 'classiﬁcation',\n",
       " 'speciﬁc',\n",
       " 'speciﬁc',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Figure',\n",
       " 'classiﬁcation',\n",
       " 'learning',\n",
       " 'preprocessing',\n",
       " 'embedding',\n",
       " 'analysis',\n",
       " 'classiﬁcation',\n",
       " 'learning',\n",
       " 'Figure',\n",
       " 'process',\n",
       " 'speciﬁc',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'process',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Figure',\n",
       " 'analysis',\n",
       " 'claims',\n",
       " 'classiﬁcation',\n",
       " 'mining',\n",
       " 'process',\n",
       " 'claims',\n",
       " 'mining',\n",
       " 'embedding',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'models',\n",
       " 'information',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Figure',\n",
       " 'insurance',\n",
       " 'information',\n",
       " 'different',\n",
       " 'called',\n",
       " 'called',\n",
       " 'Figure',\n",
       " 'insurance',\n",
       " 'insurance',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'information',\n",
       " 'information',\n",
       " 'information',\n",
       " 'information',\n",
       " 'sentence',\n",
       " 'information',\n",
       " 'information',\n",
       " 'information',\n",
       " 'information',\n",
       " 'between',\n",
       " 'different',\n",
       " 'between',\n",
       " 'information',\n",
       " 'learning',\n",
       " 'context',\n",
       " 'learning',\n",
       " 'classiﬁcation',\n",
       " 'different',\n",
       " 'called',\n",
       " 'Figure',\n",
       " 'learning',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'between',\n",
       " 'sentence',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'classiﬁcation',\n",
       " 'between',\n",
       " 'between',\n",
       " 'Figure',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'representation',\n",
       " 'between',\n",
       " 'representation',\n",
       " 'between',\n",
       " 'analysis',\n",
       " 'representation',\n",
       " 'language',\n",
       " 'called',\n",
       " 'representation',\n",
       " 'language',\n",
       " 'language',\n",
       " 'Figure',\n",
       " 'Figure',\n",
       " 'information',\n",
       " 'between',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'classiﬁcation',\n",
       " 'information',\n",
       " 'representation',\n",
       " 'representation',\n",
       " 'called',\n",
       " 'sentence',\n",
       " 'language',\n",
       " 'representation',\n",
       " 'sentence',\n",
       " 'called',\n",
       " 'speciﬁc',\n",
       " 'called',\n",
       " 'representation',\n",
       " 'mining',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'speciﬁc',\n",
       " 'context',\n",
       " 'sentence',\n",
       " 'models',\n",
       " 'analysis',\n",
       " 'different',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'models',\n",
       " 'different',\n",
       " 'Figure',\n",
       " 'speciﬁc',\n",
       " 'process',\n",
       " 'mining',\n",
       " 'claims',\n",
       " 'analysis',\n",
       " 'process',\n",
       " 'different',\n",
       " 'learning',\n",
       " 'preprocessing',\n",
       " 'preprocessing',\n",
       " 'information',\n",
       " 'Figure',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'different',\n",
       " 'preprocessing',\n",
       " 'preprocessing',\n",
       " 'representation',\n",
       " 'models',\n",
       " 'preprocessing',\n",
       " 'speciﬁc',\n",
       " 'representation',\n",
       " 'between',\n",
       " 'analysis',\n",
       " 'called',\n",
       " 'preprocessing',\n",
       " 'language',\n",
       " 'analysis',\n",
       " 'information',\n",
       " 'called',\n",
       " 'language',\n",
       " 'models',\n",
       " 'language',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'preprocessing',\n",
       " 'preprocessing',\n",
       " 'mining',\n",
       " 'preprocessing',\n",
       " 'language',\n",
       " 'Figure',\n",
       " 'preprocessing',\n",
       " 'Figure',\n",
       " 'preprocessing',\n",
       " 'preprocessing',\n",
       " 'Figure',\n",
       " 'preprocessing',\n",
       " 'preprocessing',\n",
       " 'language',\n",
       " 'speciﬁc',\n",
       " 'preprocessing',\n",
       " 'language',\n",
       " 'Figure',\n",
       " 'preprocessing',\n",
       " 'called',\n",
       " 'embedding',\n",
       " 'representation',\n",
       " 'sentence',\n",
       " 'process',\n",
       " 'called',\n",
       " 'process',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Figure',\n",
       " 'Figure',\n",
       " 'sentence',\n",
       " 'sentence',\n",
       " 'context',\n",
       " 'models',\n",
       " 'embedding',\n",
       " 'models',\n",
       " 'training',\n",
       " 'embedding',\n",
       " 'models',\n",
       " 'information',\n",
       " 'speciﬁc',\n",
       " 'speciﬁc',\n",
       " 'learning',\n",
       " 'sentence',\n",
       " 'information',\n",
       " 'information',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'speciﬁc',\n",
       " 'insurance',\n",
       " 'Figure',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'classiﬁcation',\n",
       " 'embedding',\n",
       " 'insurance',\n",
       " 'information',\n",
       " 'training',\n",
       " 'speciﬁc',\n",
       " 'training',\n",
       " 'mining',\n",
       " 'models',\n",
       " 'models',\n",
       " 'analysis',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'between',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'models',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'learning',\n",
       " 'models',\n",
       " 'models',\n",
       " 'models',\n",
       " 'between',\n",
       " 'embedding',\n",
       " 'speciﬁc',\n",
       " 'Figure',\n",
       " 'analysis',\n",
       " 'Figure',\n",
       " 'analysis',\n",
       " 'training',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'analysis',\n",
       " 'Figure',\n",
       " 'preprocessing',\n",
       " 'sentence',\n",
       " 'analysis',\n",
       " 'speciﬁc',\n",
       " 'analysis',\n",
       " 'Figure',\n",
       " 'different',\n",
       " 'sentence',\n",
       " 'called',\n",
       " 'sentence',\n",
       " 'preprocessing',\n",
       " 'called',\n",
       " 'Figure',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'speciﬁc',\n",
       " 'speciﬁc',\n",
       " 'called',\n",
       " 'different',\n",
       " 'different',\n",
       " 'analysis',\n",
       " 'Figure',\n",
       " 'learning',\n",
       " 'training',\n",
       " 'Figure',\n",
       " 'embedding',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'training',\n",
       " 'Figure',\n",
       " 'different',\n",
       " 'information',\n",
       " 'embedding',\n",
       " 'models',\n",
       " 'context',\n",
       " 'sentence',\n",
       " 'called',\n",
       " 'embedding',\n",
       " 'context',\n",
       " 'models',\n",
       " 'process',\n",
       " 'sentence',\n",
       " 'information',\n",
       " 'context',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'embedding',\n",
       " 'preprocessing',\n",
       " 'different',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'called',\n",
       " 'representation',\n",
       " 'sentence',\n",
       " 'preprocessing',\n",
       " 'sentence',\n",
       " 'representation',\n",
       " 'representation',\n",
       " 'representation',\n",
       " 'between',\n",
       " 'sentence',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'between',\n",
       " 'between',\n",
       " 'representation',\n",
       " 'context',\n",
       " 'context',\n",
       " 'context',\n",
       " 'models',\n",
       " 'Figure',\n",
       " 'models',\n",
       " 'sentence',\n",
       " 'context',\n",
       " 'sentence',\n",
       " 'context',\n",
       " 'context',\n",
       " 'training',\n",
       " 'Figure',\n",
       " 'training',\n",
       " 'context',\n",
       " 'between',\n",
       " 'training',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'context',\n",
       " 'context',\n",
       " 'between',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Figure',\n",
       " 'Figure',\n",
       " 'embedding',\n",
       " 'representation',\n",
       " 'called',\n",
       " 'information',\n",
       " 'context',\n",
       " 'called',\n",
       " 'called',\n",
       " 'Figure',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Attention',\n",
       " 'information',\n",
       " 'different',\n",
       " 'called',\n",
       " 'representation',\n",
       " 'between',\n",
       " 'training',\n",
       " 'training',\n",
       " 'sentence',\n",
       " 'Attention',\n",
       " 'between',\n",
       " 'sentence',\n",
       " 'between',\n",
       " 'context',\n",
       " 'between',\n",
       " 'Attention',\n",
       " 'models',\n",
       " 'Attention',\n",
       " 'between',\n",
       " 'between',\n",
       " 'context',\n",
       " 'Attention',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Figure',\n",
       " 'sentence',\n",
       " 'Attention',\n",
       " 'called',\n",
       " 'Attention',\n",
       " 'context',\n",
       " 'information',\n",
       " 'called',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'different',\n",
       " 'Attention',\n",
       " 'Figure',\n",
       " 'learning',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Figure',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'training',\n",
       " 'information',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Figure',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'Attention',\n",
       " 'models',\n",
       " 'Figure',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'Attention',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'sentence',\n",
       " 'representation',\n",
       " 'sentence',\n",
       " 'information',\n",
       " 'Figure',\n",
       " 'Attention',\n",
       " 'training',\n",
       " 'learning',\n",
       " 'process',\n",
       " 'models',\n",
       " 'training',\n",
       " 'training',\n",
       " 'training',\n",
       " 'language',\n",
       " 'context',\n",
       " 'language',\n",
       " 'speciﬁc',\n",
       " 'analysis',\n",
       " 'Figure',\n",
       " 'training',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'training',\n",
       " 'language',\n",
       " 'Figure',\n",
       " 'Figure',\n",
       " 'training',\n",
       " 'embedding',\n",
       " 'classiﬁcation',\n",
       " 'classiﬁcation',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'sentence',\n",
       " 'embedding',\n",
       " 'embedding',\n",
       " 'Figure',\n",
       " 'representation',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'training',\n",
       " 'training',\n",
       " 'sentence',\n",
       " 'classiﬁcation',\n",
       " 'Figure',\n",
       " 'training',\n",
       " 'classiﬁcation',\n",
       " 'sentence',\n",
       " 'sentence',\n",
       " 'classiﬁcation',\n",
       " 'Figure',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'training',\n",
       " 'language',\n",
       " 'speciﬁc',\n",
       " 'learning',\n",
       " 'speciﬁc',\n",
       " 'analysis',\n",
       " 'training',\n",
       " 'speciﬁc',\n",
       " 'classiﬁcation',\n",
       " 'representation',\n",
       " 'models',\n",
       " 'different',\n",
       " 'representation',\n",
       " 'models',\n",
       " 'models',\n",
       " 'models',\n",
       " 'models',\n",
       " 'representation',\n",
       " 'models',\n",
       " 'learning',\n",
       " 'mining',\n",
       " 'classiﬁcation',\n",
       " 'models',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'classiﬁcation',\n",
       " 'between',\n",
       " 'between',\n",
       " 'classiﬁcation',\n",
       " 'classiﬁcation',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'classiﬁcation',\n",
       " 'between',\n",
       " 'between',\n",
       " 'between',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'claims',\n",
       " 'insurance',\n",
       " 'mining',\n",
       " 'different',\n",
       " 'information',\n",
       " 'models',\n",
       " 'mining',\n",
       " 'insurance',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'information',\n",
       " 'information',\n",
       " 'learning',\n",
       " 'learning',\n",
       " 'information',\n",
       " 'learning',\n",
       " 'language',\n",
       " 'preprocessing',\n",
       " 'learning',\n",
       " 'preprocessing',\n",
       " 'learning',\n",
       " 'between',\n",
       " 'between',\n",
       " 'learning',\n",
       " 'embedding',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'language',\n",
       " 'language',\n",
       " 'training',\n",
       " 'language',\n",
       " 'classiﬁcation',\n",
       " 'information',\n",
       " 'context',\n",
       " 'learning',\n",
       " 'learning',\n",
       " 'insurance',\n",
       " 'language',\n",
       " 'language',\n",
       " 'representation',\n",
       " 'language',\n",
       " 'models',\n",
       " 'PREPRINT',\n",
       " 'OCTOBER',\n",
       " 'learning',\n",
       " 'learning',\n",
       " 'learning',\n",
       " 'Attention',\n",
       " 'between']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c298c1-7dd6-43b0-b3b2-ee1313782a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use plot method on instances of frequence distribution\n",
    "FreqDist(long_freq_words).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b202901-4d6d-4728-8d6e-9c9344b64ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
